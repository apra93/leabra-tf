{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.3.1 Comparing Accuracy Metrics\n",
    "\n",
    "The accuracy results that we've been getting so far haven't reflected the those presented in O'Reilly's paper, so let's generate them more systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "\n",
    "The following subsections are largely boilerplate code, so skip around as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 09 2019 19:34:19 \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.3.0\n",
      "\n",
      "numpy 1.16.2\n",
      "matplotlib 3.0.3\n",
      "seaborn 0.9.0\n",
      "tensorflow 1.12.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.4.0-130-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "Git hash   : 38a0a7a2722fde1a3509380782c605515e1e71bf\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and packages. Add more as necessary.\n",
    "%watermark -v -n -m -g -b -t -p numpy,matplotlib,seaborn,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static imports that shouldn't necessarily change throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "from functools import reduce\n",
    "\n",
    "# Third party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local imports that may or may not be autoreloaded. This section contains things that will likely have to be re-imported multiple times, and have additions or subtractions made throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task script\n",
    "%aimport leabratf.tasks.combinatorics.combigen\n",
    "import leabratf.tasks.combinatorics.combigen as cg\n",
    "# Visualization for the task\n",
    "%aimport leabratf.visualization.combigen_heatmap\n",
    "import leabratf.visualization.combigen_heatmap as cgh\n",
    "# Utility functions\n",
    "%aimport leabratf.utils\n",
    "from leabratf.utils import setup_logging\n",
    "%aimport leabratf.constants\n",
    "from leabratf.constants import DIR_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set [seaborn defaults](https://seaborn.pydata.org/generated/seaborn.set.html) for matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the logger configuration to something more useful than baseline. Creates log files for the different log levels in the `logs` directory.\n",
    "\n",
    "See `logging.yml` for the exact logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run base logger setup\n",
    "setup_logging()\n",
    "# Define a logger object\n",
    "logger = logging.getLogger('leabratf')\n",
    "# Don't propagate messages\n",
    "logger.propagate = False\n",
    "\n",
    "tf.logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combigen Task Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of slots in a training set\n",
    "STACK = 4\n",
    "# Size of each axis in the input array\n",
    "SIZE = 5\n",
    "# Number of axes to use per slot\n",
    "DIMS = 2\n",
    "# Number of lines per axis\n",
    "LINES = [1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train for\n",
    "EPOCHS = 500\n",
    "# Number of samples in the training set\n",
    "N_TRAIN= 100\n",
    "# Number of samples in the validation set\n",
    "N_VAL = 50\n",
    "# Number of samples in the testing set\n",
    "N_TEST = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "Y_TRAIN = cg.generate_labels(n_samples=N_TRAIN, stack=STACK, size=SIZE, dims=DIMS, n_lines=LINES)\n",
    "X_TRAIN = cg.inverse_transform(Y_TRAIN)\n",
    "# Validation Data\n",
    "Y_VAL = cg.generate_labels(n_samples=N_VAL, stack=STACK, size=SIZE, dims=DIMS, n_lines=LINES)\n",
    "X_VAL = cg.inverse_transform(Y_VAL)\n",
    "# Testing data\n",
    "Y_TEST = cg.generate_labels(n_samples=N_TEST, stack=STACK, size=SIZE, dims=DIMS, n_lines=LINES)\n",
    "X_TEST = cg.inverse_transform(Y_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "ALPHA = 0.01\n",
    "# Batch size\n",
    "BATCH_SIZE = 1\n",
    "# Number of parameters in the inputs\n",
    "N_INPUTS = STACK * SIZE ** DIMS\n",
    "# Number of hidden units\n",
    "N_HIDDEN_1 = 100\n",
    "# Number of parameters in the labels\n",
    "N_OUTPUTS = STACK * SIZE * DIMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to print an update\n",
    "N_UPDATES = 5\n",
    "# Which device to train on\n",
    "TF_DEVICE = '/cpu:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This section goes over some of the background information for the notebook using results from previous notebooks. Skip around as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Combigen Task\n",
    "\n",
    "Quickly remind ourselves what the task looks like before diving in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cgh.visualize_combigen(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Accuracy Results\n",
    "\n",
    "Between nb-0.2.x and 0.3, our accuracy results have been quite reproducible. Here is a summary of the different results so far. All of the models shown below were trained using the O'Reilly BP architecture and binary cross-entropy as the loss. \n",
    "\n",
    "#### Elemental Accuracy\n",
    "\n",
    "In nb-0.2.x, the O'Reilly BP model was implemented in keras using an absolute error based accuracy metric. Below is the training curve when trained to 5000 epochs.\n",
    "\n",
    "![BCE_SGD](images/nb0.2.1_bce_sgd_5000_epochs.png)\n",
    "\n",
    "#### Sample Accuracy\n",
    "\n",
    "In nb-0.3, the model was implemented in tensorflow with a modification to the accuracy\n",
    "Some things to note are that the models were trained to 10 times the number of epochs as O'Reilly did (500). However, a dotted vertical line was added at the 500 epoch mark to make it clear what the model's performance would have looked like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Metrics\n",
    "\n",
    "This next section will define the computational graph that will be used to generate the metrics down below. It is largely code copied from nb-0.3, so skip around as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup any residual nodes\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Datasets\n",
    "\n",
    "Define the various `tf.Dataset`s that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_datasets(init_ops):\n",
    "    # The first step of the setup is that each of the datasets (training, validation, and \n",
    "    # testing) are turned into their own `Dataset` objects.\n",
    "    # Training dataset\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_TRAIN, Y_TRAIN)).repeat().batch(BATCH_SIZE)\n",
    "    # Validation dataset\n",
    "    dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_VAL, Y_VAL)).repeat().batch(N_VAL)\n",
    "    # Testing dataset\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_TEST, Y_TEST)).repeat().batch(N_TEST)\n",
    "    \n",
    "    # Next, let's define the iterators for each of the datasets, and then add their \n",
    "    # initializations to the `init_ops` list.\n",
    "    # Training iterator\n",
    "    train_iter = dataset_train.make_initializable_iterator()\n",
    "    # Validation iterator\n",
    "    val_iter = dataset_val.make_initializable_iterator()\n",
    "    # Testing iterator\n",
    "    test_iter = dataset_test.make_initializable_iterator()\n",
    "    # Aggregate the iterators\n",
    "    iterators = [train_iter, val_iter, test_iter]\n",
    "\n",
    "    # Add the initiatlizations to the init opts\n",
    "    init_ops += [train_iter.initializer, val_iter.initializer, test_iter.initializer]\n",
    "    \n",
    "    # And finally, the interesting part. Rather than creating separate next elements for \n",
    "    # the model, the `tf.data` API has a string handler iterator so we can contextually \n",
    "    # switch the active `Dataset` object, resulting in different values being used for `x` \n",
    "    # and `y`.\n",
    "\n",
    "    # The way this is done is by defining a `tf.placeholder` variable, which is used \n",
    "    # first to create a string handler iterator, and later to hold the dataset-indicating \n",
    "    # string handle. The string handler iterator is what then changes the values of `x` and \n",
    "    # `y`, naturally also supplying them using the `get_next` method.\n",
    "    # The placeholder variable of type string\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    # Iterator from string handle\n",
    "    handle_iterator = tf.data.Iterator.from_string_handle(\n",
    "        handle, dataset_train.output_types, \n",
    "        dataset_train.output_shapes)\n",
    "\n",
    "    # x and y that will be used in the graph\n",
    "    x, y = handle_iterator.get_next()\n",
    "    \n",
    "    return x, y, iterators, handle, init_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Variables\n",
    "\n",
    "Straight forward section where we define the weights and biases. One thing to note is that the weights are initialized using the `tf.contrib.layers.xavier_initializer`. \n",
    "\n",
    "Additionally, create an empty list that will contain the initialization operations to be performed at the start of a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "weights = {\n",
    "    'h1': tf.get_variable(name='w_h1', shape=[N_INPUTS, N_HIDDEN_1],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'out': tf.get_variable(name='w_out', shape=[N_HIDDEN_1, N_OUTPUTS],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer()),\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.get_variable(name=\"b_1\", shape=[N_HIDDEN_1], \n",
    "                      initializer=tf.zeros_initializer()),\n",
    "    'out': tf.get_variable(name=\"b_out\", shape=[N_OUTPUTS], \n",
    "                      initializer=tf.zeros_initializer()),\n",
    "}\n",
    "\n",
    "# List for initialization operations\n",
    "init_ops = [tf.global_variables_initializer()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Loss Function\n",
    "\n",
    "The architecture is the same as previous notebooks. See nb-0.3 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oreilly_model(inputs):\n",
    "    # Reshape for hidden layer\n",
    "    inputs = tf.reshape(inputs, shape=[-1, N_INPUTS])\n",
    "    # Single hidden layer\n",
    "    inputs = tf.sigmoid(tf.add(tf.matmul(inputs, weights['h1']), biases['b1']))\n",
    "    # Output layer\n",
    "    inputs = tf.add(tf.matmul(inputs, weights['out']), biases['out'])\n",
    "    # Reshape for labels\n",
    "    return tf.reshape(inputs, shape=[-1, STACK, SIZE, DIMS])\n",
    "\n",
    "with tf.device(TF_DEVICE):\n",
    "    # Get the relevant dataset nodes\n",
    "    x, y, iterators, handle, init_ops = make_tf_datasets(init_ops)\n",
    "    \n",
    "    # Build the model\n",
    "    logits = oreilly_model(x)\n",
    "    # Ensure y is cast to the same type as logits\n",
    "    labels = tf.cast(y, logits.dtype)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=ALPHA)\n",
    "\n",
    "    # train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The last few ops to define before training are the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(TF_DEVICE):\n",
    "    # Define some intermediate nodes\n",
    "    predictions = tf.sigmoid(logits)\n",
    "    rounded_predictions = tf.round(predictions)\n",
    "    equal_labels_and_preds = tf.equal(rounded_predictions, labels)\n",
    "    \n",
    "    # O'Reilly Accuracy \n",
    "    axis_acc = tf.reduce_all(equal_labels_and_preds, axis=(2))\n",
    "    slot_acc = tf.reduce_all(axis_acc, axis=(2))\n",
    "    sample_acc = tf.reduce_all(slot_acc, axis=(1))\n",
    " \n",
    "    # Elemental Accuracy \n",
    "    el_acc_op = tf.reduce_mean(tf.cast(equal_labels_and_preds, tf.float16))\n",
    "    # Axis Accuracy\n",
    "    axis_acc_op = tf.reduce_mean(tf.cast(axis_acc, tf.float16))\n",
    "    # Slot Accuracy\n",
    "    slot_acc_op = tf.reduce_mean(tf.cast(slot_acc, tf.float16))\n",
    "    # Sample Accuracy\n",
    "    sample_acc_op = tf.reduce_mean(tf.cast(sample_acc, tf.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Session Function\n",
    "\n",
    "In the event that we do not want to immediately close sessions, we won't be using the context handler but will still need to grab new sessions as necessary. So let's quickly write a function that will properly run `sess.close()` if a previous session exists and then return a new `tf.Session` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_session(*args, **kwargs):\n",
    "    # Try to close the globally defined session if it isn't already\n",
    "    try:\n",
    "        if not sess._closed:\n",
    "            sess.close()\n",
    "    # If it doesn't exist, then just pass\n",
    "    except NameError:\n",
    "        pass\n",
    "    # Return the new instance\n",
    "    return tf.Session(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Training Function\n",
    "\n",
    "Let's define a function that runs the training routine and accepts the number of epochs as the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or_model(sess, epochs=EPOCHS, n_updates=N_UPDATES):\n",
    "    # Ensure this is an int\n",
    "    epochs = int(epochs)\n",
    "    # Dict with the various metrics we care about\n",
    "    metrics = {'loss':[], 'el_acc':[], 'ax_acc':[], 'sl_acc':[], 'sm_acc':[], \n",
    "               'val_loss':[], 'val_el_acc':[], 'val_ax_acc':[], 'val_sl_acc':[], \n",
    "               'val_sm_acc':[]}\n",
    "    # Run the initialization ops\n",
    "    sess.run(init_ops)\n",
    "    # Define training and validation handlers\n",
    "    training_handle, validation_handle, testing_handle = sess.run([i.string_handle() \n",
    "                                                                   for i in iterators])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ep_loss, ep_el_acc, ep_sl_acc, ep_ax_acc, ep_sm_acc = np.zeros([5, N_TRAIN])\n",
    "        # Run the training steps\n",
    "        for i in range(N_TRAIN):\n",
    "            _, ep_loss[i], ep_el_acc[i], ep_sl_acc[i], ep_ax_acc[i], ep_sm_acc[i] = sess.run(\n",
    "                [train_op, loss_op, el_acc_op, axis_acc_op, slot_acc_op, sample_acc_op],\n",
    "                feed_dict={handle: training_handle})\n",
    "            \n",
    "        # Get means for the epoch\n",
    "        epoch_data = list(np.mean((ep_loss, ep_el_acc, ep_sl_acc, ep_ax_acc, ep_sm_acc), \n",
    "                                  axis=1))\n",
    "\n",
    "        # Calculate validation accuracy and loss\n",
    "        val_data = list(sess.run([loss_op, el_acc_op, axis_acc_op, slot_acc_op, sample_acc_op],\n",
    "                                 feed_dict={handle: validation_handle}))\n",
    "        \n",
    "        # Record\n",
    "        for key, value in zip(metrics.keys(), epoch_data + val_data):\n",
    "            metrics[key].append(value)\n",
    "\n",
    "        # Selectively display the epoch number\n",
    "        if not epoch % (epochs / n_updates) or epoch == epochs - 1:\n",
    "            print((\"Completed epoch {0}/{1}. Metrics:\\n\" + \n",
    "                   \"                     Loss   Sample Accuracy   Elem Accuracy\\n\" +\n",
    "                   \"    Epoch:      {2:10.4f}   {3:10.4f}   {4:10.4f}\\n\" +\n",
    "                   \"    Validation: {5:10.4f}   {6:10.4f}   {7:10.4f}\\n\").format(\n",
    "                epoch+1, epochs, epoch_data[0],epoch_data[4], epoch_data[1], \n",
    "                val_data[0], val_data[4], val_data[1]))\n",
    "\n",
    "    # Calculate accuracy for test images\n",
    "    print(\"Optimization Finished! Testing Accuracy:\", sess.run(\n",
    "        accuracy_op, feed_dict={handle: testing_handle}))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Metrics for 10 Models\n",
    "\n",
    "The last thing to do is encapsulate everything into one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for model 0...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.6657       0.0000       0.5955\n",
      "    Validation:     0.6367       0.0000       0.6450\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0635       0.3999       0.9766\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.454\n",
      "Completed training for model 0!\n",
      "\n",
      "Starting training for model 1...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.7226       0.0000       0.5315\n",
      "    Validation:     0.6821       0.0000       0.5732\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0618       0.4199       0.9771\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.484\n",
      "Completed training for model 1!\n",
      "\n",
      "Starting training for model 2...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.6739       0.0000       0.6012\n",
      "    Validation:     0.6468       0.0000       0.6465\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0613       0.3999       0.9761\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.452\n",
      "Completed training for model 2!\n",
      "\n",
      "Starting training for model 3...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.7103       0.0000       0.5833\n",
      "    Validation:     0.6879       0.0000       0.6040\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0674       0.3799       0.9736\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.438\n",
      "Completed training for model 3!\n",
      "\n",
      "Starting training for model 4...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.7036       0.0000       0.5950\n",
      "    Validation:     0.6646       0.0000       0.6196\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0596       0.4199       0.9775\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.454\n",
      "Completed training for model 4!\n",
      "\n",
      "Starting training for model 5...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.7352       0.0000       0.4913\n",
      "    Validation:     0.6886       0.0000       0.5649\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0639       0.3601       0.9746\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.49\n",
      "Completed training for model 5!\n",
      "\n",
      "Starting training for model 6...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.7234       0.0000       0.4922\n",
      "    Validation:     0.6932       0.0000       0.5278\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0585       0.4800       0.9790\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.466\n",
      "Completed training for model 6!\n",
      "\n",
      "Starting training for model 7...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.6881       0.0000       0.5540\n",
      "    Validation:     0.6663       0.0000       0.5747\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0644       0.5200       0.9790\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.434\n",
      "Completed training for model 7!\n",
      "\n",
      "Starting training for model 8...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.6645       0.0000       0.5910\n",
      "    Validation:     0.6521       0.0000       0.6362\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0631       0.5000       0.9761\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.468\n",
      "Completed training for model 8!\n",
      "\n",
      "Starting training for model 9...\n",
      "Completed epoch 1/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.6865       0.0000       0.5622\n",
      "    Validation:     0.6628       0.0000       0.6001\n",
      "\n",
      "Completed epoch 25000/25000. Metrics:\n",
      "                     Loss   Sample Accuracy   Elem Accuracy\n",
      "    Epoch:          0.0025       1.0000       1.0000\n",
      "    Validation:     0.0621       0.3601       0.9746\n",
      "\n",
      "Optimization Finished! Testing Accuracy: 0.452\n",
      "Completed training for model 9!\n",
      "\n",
      "CPU times: user 1d 3h 11min 44s, sys: 4h 45min 4s, total: 1d 7h 56min 49s\n",
      "Wall time: 11h 19min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sess = new_session()\n",
    "metrics_ = []\n",
    "for i in range(10):\n",
    "    print('Starting training for model {0}...'.format(i))\n",
    "    metrics.append(train_or_model(sess, epochs=50*EPOCHS, n_updates=1))\n",
    "    print('Completed training for model {0}!\\n'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Metrics\n",
    "\n",
    "Just like before, let's plot the training curves using the metrics we obtained. To do this, let's borrow the same function from nb-0.3 but make it a bit more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leabratf.utils import as_list\n",
    "\n",
    "def plot_metrics(metrics_inp, metrics=None, epochs=None):\n",
    "    import ipdb;ipdb.set_trace()\n",
    "    metrics_list = [metrics_inp] if isinstance(metrics_inp, dict) else list(metrics_inp)\n",
    "    for metrics_dict in metrics_list:\n",
    "        # Keep this working for keras\n",
    "        metrics_dict = metrics_dict if not hasattr(metrics_dict, 'history') else metrics_dict.history\n",
    "        # What metrics to plot\n",
    "        metrics = metrics or metrics_dict.keys()\n",
    "        # How many epochs to plot\n",
    "        if not epochs:\n",
    "            len_metrics = [len(val) for val in metrics_dict.values()]\n",
    "            assert all(l == len_metrics[0] for l in len_metrics), 'Metrics have different lengths'\n",
    "            epochs = len_metrics[0]\n",
    "        # Plot each metric\n",
    "        for key in metrics:\n",
    "            plt.plot(metrics_dict[key][:epochs], label=key)\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epochs')\n",
    "    if epochs > EPOCHS:\n",
    "        plt.axvline(EPOCHS, linestyle='--', label='500 Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_metrics(metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Legend\n",
    "\n",
    "Now that we have so may metrics to keep track of, it's worth taking the time to define each of them.\n",
    "\n",
    "#### Training Set Metrics\n",
    "\n",
    "Metrics obtained every epoch from from performance on the training set.\n",
    "\n",
    "- `loss` - Loss for the training set\n",
    "- `el_acc` - Element-wise accuracy between predictions and labels for the training set\n",
    "- `ax_acc` - Axis accuracy for each slot in each sample between predictions and labels for the training set. Score per sample goes in eighth steps between 0.0 and 1.0. Every correct axis in every slot contributes 0.125 to the the overall accuracy\n",
    "- `sl_acc` - Slot accuracy for sample between predictions and labels for the training set. Score per sample goes in quarter steps between 0.0 and 1.0. Any slot that has all elements correct contributes 0.25 to the score\n",
    "- `sm_acc` - Sample accuracy between predictions and labels for the training set. Score per sample is binary - model must get every element correct to receive 1.0\n",
    "\n",
    "#### Validation Set Metrics\n",
    "\n",
    "Metrics obtained every epoch from from performance on the validation set.\n",
    "\n",
    "- `val_loss` - Loss for the validation set for a particular epoch\n",
    "- `val_el_acc` - Element-wise accuracy between predictions and labels for the validation set. See above for details\n",
    "- `val_sl_acc` - Slot accuracy for sample between predictions and labels for the validation set. See above for details\n",
    "- `val_sm_acc` - Sample accuracy between predictions and labels for the training set. See above for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Metrics\n",
    "\n",
    "Let's plot the average of all the metrics in one plot just to see the general structure, so first get an average value dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ave = {key:np.mean([model[key] for model in metrics], axis=0) for key in metrics[0].keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plot them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-0723d4b4b028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_ave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-4952321d8a00>\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m(metrics_inp, metrics, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmetrics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# What metrics to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# How many epochs to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "plot_metrics(metrics_ave)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics in Detail\n",
    "\n",
    "Let's go through each of the different accuracy metrics individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Accuracy\n",
    "\n",
    "The first one to check out should be the sample accuracy since it's the one that seems the most similar to O'Reilly's definition of accuracy. So let's plot the sample accuracies of all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
