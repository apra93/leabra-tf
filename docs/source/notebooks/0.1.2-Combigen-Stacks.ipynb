{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1.2 Combigen Stacks\n",
    "\n",
    "When putting the model together, I realized the O'Reily task requires that groups of four inputs be sent into the models. So this notebook will go theough the process of modifying and ensuring the task does this properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 19 2019 00:21:34 \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.2.0\n",
      "\n",
      "numpy 1.15.4\n",
      "matplotlib 3.0.2\n",
      "seaborn 0.9.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-45-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 71e15f127ed90ba0b13002fd4882983f74176e16\n",
      "Git branch : resnet\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and packages. Add more as necessary.\n",
    "%watermark -v -n -m -g -b -t -p numpy,matplotlib,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static imports that shouldn't necessarily change throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "# Third party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local imports that may or may not be autoreloaded. This section contains things that will likely have to be re-imported multiple times, and have additions or subtractions made throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task script\n",
    "%aimport leabratf.tasks.combinatorics.combigen\n",
    "import leabratf.tasks.combinatorics.combigen as cbg\n",
    "# Visualization for the task\n",
    "%aimport leabratf.visualization.combigen_heatmap\n",
    "import leabratf.visualization.combigen_heatmap as cbhm\n",
    "# Utility functions\n",
    "%aimport leabratf.utils\n",
    "from leabratf.utils import setup_logging, make_input_3d, flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set [seaborn defaults](https://seaborn.pydata.org/generated/seaborn.set.html) for matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the logger configuration to something more useful than baseline. Creates log files for the different log levels in the `logs` directory.\n",
    "\n",
    "See `logging.yml` for the exact logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run base logger setup\n",
    "setup_logging()\n",
    "# Define a logger object\n",
    "logger = logging.getLogger(\"leabratf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Labels\n",
    "\n",
    "For more detail on the task, check out nb-0.1 and nb-0.1.1. For now, let's instantiate local versions of the relevant combigen functions so we can start modifying them.\n",
    "\n",
    "First let's start the label generation function, `generate_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(n_samples=1, size=5, dims=2):\n",
    "    \"\"\"...\"\"\"\n",
    "    # Generate baseline labels\n",
    "    raw_labels = np.random.choice(2, (n_samples, size, dims), replace=True)\n",
    "    # Random selection of indices to zero out\n",
    "    arg_zero = np.random.choice(size, (n_samples*dims), replace=True)\n",
    "    # Alternating indices to loop through the dims of the labels\n",
    "    dim_indices = np.tile(range(dims), n_samples)\n",
    "    # Repeating indices to loop through the samples\n",
    "    sample_indices = np.repeat(range(n_samples), dims)\n",
    "    \n",
    "    # Zero out a random selection of indices\n",
    "    raw_labels[sample_indices, arg_zero, dim_indices] = 0\n",
    "    return raw_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's just see what a resulting label look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_y = generate_labels()\n",
    "cbhm.heatmap(example_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's make a better version that will include stacks, and see what it outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels_stacked(n_samples=1, stack=4, size=5, dims=2):\n",
    "    \"\"\"...\"\"\"\n",
    "    # Generate baseline labels\n",
    "    raw_labels = np.random.choice(2, (n_samples, stack, size, dims), replace=True)\n",
    "    # Random selection of indices to zero out\n",
    "    arg_zero = np.random.choice(size, (n_samples*dims*stack), replace=True)\n",
    "    # Alternating indices to loop through the dims of the labels\n",
    "    dim_indices = np.tile(range(dims), stack*n_samples)\n",
    "    # Repeating indices to loop through the samples\n",
    "    sample_indices = np.repeat(range(n_samples), dims*stack)\n",
    "    # Stack indices\n",
    "    stack_indices = np.repeat(np.tile(range(stack), n_samples), dims)\n",
    "    \n",
    "    # Zero out a random selection of indices\n",
    "    raw_labels[sample_indices, stack_indices, arg_zero, dim_indices] = 0\n",
    "    return raw_labels\n",
    "\n",
    "stacked_y = generate_labels_stacked()\n",
    "print(stacked_y.shape)\n",
    "print(stacked_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far, lets run a couple tests to ensure the zeroing is working properly. First check that it trivially returns arrays of the requested shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of shapes that will be used to test the labels\n",
    "test_shapes_01 = [[1, 4, 5, 2], [10, 4, 5, 2], [1,1,1,1], [10,10,10,10]]\n",
    "\n",
    "# Test the resulting shape\n",
    "for shape in tqdm(test_shapes_01):\n",
    "    assert np.array_equal(generate_labels_stacked(*shape).shape, shape)\n",
    "logger.info('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make sure the zeroing fix is still working for all stacks. Borrowing and tweaking the function in nb-0.1.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_label_counts(func=generate_labels_stacked, n_labels=1000000, *args, **kwargs):\n",
    "    \"\"\"Genrates `n_labels` labels and checks their sums.\"\"\"\n",
    "    # Generate a large number of y values to test\n",
    "    large_test_Y = func(n_labels, *args, **kwargs)\n",
    "    \n",
    "    n, stacks, size, dims = large_test_Y.shape\n",
    "    assert n == n_labels\n",
    "    \n",
    "    # Sum over the long dimension of each sample to see how many of them are set to\n",
    "    # the on state. If they are all on, then it will sum to the length of the dim.\n",
    "    label_sums = np.sum(large_test_Y, axis=2)\n",
    "    \n",
    "    expected_values_in_sum = np.isin(range(size + 1), label_sums)\n",
    "    assert len(expected_values_in_sum) == len(range(size + 1))\n",
    "    \n",
    "    # Get unique values in the sum and their counts and put them in a dict\n",
    "    count_dict = {val:count for val, count in zip(\n",
    "        *np.unique(label_sums, return_counts=True))}\n",
    "    # Sanity check\n",
    "    assert sum(count_dict.values()) == n_labels * dims * stacks\n",
    "    \n",
    "    # Perform the actual check\n",
    "    assert all(expected_values_in_sum == size*[True] + [False])    \n",
    "\n",
    "# A list of shapes that will be used to test the labels\n",
    "test_shapes_02 = [[4, 5, 2], [1, 1, 1], [5, 5, 5]]\n",
    "\n",
    "for shape in tqdm(test_shapes_02):\n",
    "    test_all_label_counts(generate_labels_stacked, 1000000, *shape)\n",
    "logger.info('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for Stacked Labels\n",
    "\n",
    "Now let's try to get the heatmap to work with the new shape of the labels. First let's show the old implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General warning, this will overwrite the originally defined heatmap\n",
    "@make_input_3d\n",
    "def heatmap(data, vmin=0, vmax=2, cbar=False, linewidths=1, square=True, \n",
    "            samples_per_row=10, gridspec_kw=None, sharey=True, sharex=True,\n",
    "            titles=None, *args, **kwargs):\n",
    "    \"\"\"See cbhm.heatmap for the full docstring\"\"\"\n",
    "    # Place them all in a subplot\n",
    "    n_samples = len(data)\n",
    "    ver_size = n_samples // samples_per_row\n",
    "    ver_size = ver_size + 1 if n_samples % samples_per_row else ver_size\n",
    "    \n",
    "    hor_size = (samples_per_row \n",
    "                if (ver_size > 1 or n_samples == samples_per_row) \n",
    "                else n_samples % samples_per_row)\n",
    "    # Create the subplot axes\n",
    "    _, axn = plt.subplots(ver_size, hor_size, sharey=sharey, sharex=sharex,\n",
    "                          squeeze=False, gridspec_kw=gridspec_kw)\n",
    "\n",
    "    # Create a titles generator\n",
    "    if titles is not None:\n",
    "        titles = iter(titles)\n",
    "    else:\n",
    "        titles = count(start=0, step=1)\n",
    "    # Collect the returned heatmap parameters\n",
    "    heatmaps = []\n",
    "\n",
    "    # Loop through and generate the plots\n",
    "    gen_data = iter(data)\n",
    "    for i in range(ver_size):\n",
    "        try:\n",
    "            for j in range(hor_size):\n",
    "                array = next(gen_data)\n",
    "                shape = array.shape\n",
    "                if len(shape) == 3 and shape[0] == 1:\n",
    "                    array = array.reshape(shape[1:])\n",
    "                heatmaps.append(sns.heatmap(\n",
    "                    array, vmin=vmin, vmax=vmax, cbar=cbar,\n",
    "                    linewidths=linewidths, square=square, ax=axn[i,j], *args,\n",
    "                    **kwargs))\n",
    "                axn[i,j].set_title(next(titles))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's redefine it with some changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "def heatmap_stacked(data, data2=None, vmin=0, vmax=2, cbar=False, linewidths=1, square=True, \n",
    "            samples_per_row=10, gridspec_kw=None, sharey=True, sharex=True,\n",
    "            titles=None, squeeze=False, y_label='Samples', x_label=None, *args, **kwargs):\n",
    "    \"\"\"See cbhm.heatmap for the full docstring\"\"\"\n",
    "    n_samples, stack, size, dims = data.shape\n",
    "    # Check if another dataset was passed\n",
    "    hor = stack if data2 is None else stack + data2.shape[1]\n",
    "    # Place everything in subplots\n",
    "    fig, axn = plt.subplots(n_samples, hor, sharey=sharey, sharex=sharex,\n",
    "                            squeeze=squeeze, gridspec_kw=gridspec_kw)\n",
    "    if x_label is not None:\n",
    "        fig.suptitle(x_label)\n",
    "\n",
    "    # Create a titles generator\n",
    "    if titles is not None:\n",
    "        titles = iter(titles)\n",
    "    else:\n",
    "        titles = count(start=0, step=1)\n",
    "    # Collect the returned heatmap parameters\n",
    "    heatmaps = []\n",
    "\n",
    "    # Loop through and generate the plots\n",
    "    gen_data = iter(data) if data2 is None else zip(data, data2)\n",
    "    \n",
    "    for i, sample in enumerate(gen_data):\n",
    "        if data2 is None:\n",
    "            stacks = [stack for stack in sample]\n",
    "        else:\n",
    "            stacks = [stack for data in sample for stack in data]\n",
    "                \n",
    "        for j, stack in enumerate(stacks):\n",
    "            if len(stack.shape) == 3 and stack.shape[0] == 1:\n",
    "                stack = stack.reshape(stack.shape[1:])\n",
    "                \n",
    "            heatmaps.append(sns.heatmap(\n",
    "                stack, vmin=vmin, vmax=vmax, cbar=cbar, xticklabels=False,\n",
    "                yticklabels=False,\n",
    "                linewidths=linewidths, square=square, ax=axn[i,j], *args,\n",
    "                **kwargs))\n",
    "            axn[i,j].set_title(next(titles))\n",
    "            \n",
    "            if j is 0:\n",
    "                axn[i,j].set_ylabel(i, rotation=0)\n",
    "    \n",
    "    # Common Labels\n",
    "    if y_label is not None:\n",
    "        fig.text(0.05, 0.5, y_label, va='center', rotation='vertical')\n",
    "    \n",
    "    return heatmaps\n",
    "\n",
    "N = 2\n",
    "heatmap_stacked(generate_labels_stacked(N))\n",
    "heatmap_stacked(generate_labels_stacked(N),generate_labels_stacked(N))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Inverse Transform \n",
    "\n",
    "And now let's rework `inverse_transform`. Below is the old implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_single_sample(y):\n",
    "    \"\"\"...\"\"\"\n",
    "    # Grab the length of y\n",
    "    size, _ = y.shape\n",
    "    # Create a horizontal array and a vertical array according to y\n",
    "    horizontal, vertical = np.tile(y, size).reshape(size, size, 2).T\n",
    "    return (horizontal.T + vertical).astype(bool).astype(np.float32)\n",
    "\n",
    "@make_input_3d\n",
    "def inverse_transform(Y, *args, **kwargs):\n",
    "    \"\"\"...\"\"\"\n",
    "    length, size, dims = Y.shape\n",
    "    return np.concatenate([inverse_transform_single_sample(y) for y in Y],\n",
    "                          axis=0).reshape(length, *([size]*dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's make it a stacked implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_stacked(Y, *args, **kwargs):\n",
    "    \"\"\"...\"\"\"\n",
    "    n_samples, stack, size, dims = Y.shape\n",
    "    return np.concatenate([inverse_transform_single_sample(face) \n",
    "                           for y in Y for face in y], axis=0).reshape(\n",
    "        n_samples, stack, *([size]*dims))\n",
    "\n",
    "heatmap_stacked(inverse_transform_stacked(generate_labels_stacked(2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's test to make sure the inverse transform is still correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shapes_03 = [[1, 4, 5, 2]]#, [10, 4, 5, 2], [10,10,10,2]]\n",
    "\n",
    "for shape in tqdm(test_shapes_03):\n",
    "    example_y = generate_labels_stacked(*shape)\n",
    "    n_samples, stack, size, dim = example_y.shape\n",
    "    expected_X = np.concatenate(\n",
    "        np.array([inverse_transform(y).reshape(1, stack, size, size)\n",
    "                  for y in example_y]), axis=0)\n",
    "    generated_X = inverse_transform_stacked(example_y)\n",
    "    assert np.array_equal(expected_X, generated_X)    \n",
    "logger.info('Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_x = np.concatenate(\n",
    "        np.array([inverse_transform(y).reshape(1, stack, size, size)\n",
    "                  for y in example_y]), axis=0)\n",
    "example_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Combigen Visualization\n",
    "\n",
    "The last function that needs to be changed is `visualize_combigen`, so just like before let's show the original implmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_combigen(n_pairs=1, *args, **kwargs):\n",
    "    \"\"\"...\"\"\"\n",
    "    heatmaps = []\n",
    "    # Visulize a few combinations of x and y\n",
    "    for _ in range(n_pairs):\n",
    "        # Generate a signle y\n",
    "        example_y = cg.generate_labels(n_samples=1, *args, **kwargs)\n",
    "        # Generate a single x from the y\n",
    "        example_x = cg.inverse_transform(example_y)\n",
    "        heatmaps.append(heatmap([example_y, example_x[0]],\n",
    "                                gridspec_kw={'width_ratios': [2, 5]},\n",
    "                                sharex=False,\n",
    "                                titles=['y', 'X']))\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the changed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_combigen_stacked(n_pairs=2, *args, **kwargs):\n",
    "    \"\"\"...\"\"\"\n",
    "    heatmaps = []\n",
    "    # Generate a signle y\n",
    "    example_y = generate_labels_stacked(n_samples=n_pairs, *args, **kwargs)\n",
    "    # Generate a single x from the y\n",
    "    example_x = inverse_transform_stacked(example_y)\n",
    "    \n",
    "    _, stack, size, dim = example_y.shape\n",
    "    titles = [a+str(i) for a in ['y','x'] for i in range(4)] + ['']*(n_pairs-1)*stack*2\n",
    "    gridspec_kw={'width_ratios': [dim]*stack + [size]*stack}\n",
    "    \n",
    "    heatmaps.append(heatmap_stacked(example_y, example_x,\n",
    "                                    gridspec_kw=gridspec_kw,\n",
    "                                    sharex=False,\n",
    "                                    titles=titles,\n",
    "                                    x_label='y and X Pairs'))\n",
    "    return heatmaps\n",
    "\n",
    "visualize_combigen_stacked()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And once more with more pairs\n",
    "visualize_combigen_stacked(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Last thing to check for is the performance change with the added implementation. Let's compare the new implementation compared to the old one when required to generate four times as much data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unstacked_data(n_samples=1000):\n",
    "    Y = generate_labels(n_samples*4)\n",
    "    x = inverse_transform(Y)\n",
    "    return x, Y\n",
    "\n",
    "def generate_stacked_data(n_samples=1000):\n",
    "    Y = generate_labels_stacked(n_samples)\n",
    "    x = inverse_transform_stacked(Y)\n",
    "    return x, Y\n",
    "\n",
    "# Now show the timing\n",
    "print('Timing of old implementation:')\n",
    "%timeit generate_unstacked_data()\n",
    "print('Timing of stacked implementation:')\n",
    "%timeit generate_stacked_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More or less identical."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
