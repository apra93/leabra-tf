

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>0.4.1 Exploring Different Optimizers &mdash; Leabra Tensorflow 0+untagged.52.gf1b6772 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="0.5 Adam Optimizer Learning Rate Tuning and Dropout" href="0.5-Adam-Optimizer-Learning-Rate-Tuning-and-Dropout.html" />
    <link rel="prev" title="0.4 Exploring Different Learning Rates" href="0.4-Exploring-Different-Learning-Rates.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Leabra Tensorflow
          

          
          </a>

          
            
            
              <div class="version">
                0+untagged.52.gf1b6772
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../release_schedule.html">Release Schedule</a></li>
</ul>
<p class="caption"><span class="caption-text">Tasks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../combinatorics.html">Combinatorial Generalizaton and Interactivity</a></li>
</ul>
<p class="caption"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../visualization.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.html">Utility Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tfutils.html">Tensorflow Utility Functions</a></li>
</ul>
<p class="caption"><span class="caption-text">Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="0.0-Notebook-Template.html">0.0 Leabra-tf Notebook Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.1-Designing-the-Combigen-Task.html">0.1 Implementing the Combinatorial Generalization Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.1.1-Combigen-All-True-Bug-Fix.html">0.1.1 Combigen All-True Bug Fix</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.1.2-Combigen-Stacks.html">0.1.2 Combigen Stacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.1.3-Specifying-the-Number-of-Lines.html">0.1.3 Specifying the Number of Lines</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.1.4-Selectable-Line-Statistics.html">0.1.4 Selectable Line Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.2-Running-a-Basic-Model-on-Combigen.html">0.2 Running a Basic Model on Combigen</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.2.1-Iterating-the-BP-Model-on-GPUs.html">0.2.1 Iterating the BP Model on GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.3-BP-Model-in-Tensorflow.html">0.3 BP Model in Tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.3.1-Comparing-Accuracy-Metrics.html">0.3.1 Comparing Accuracy Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.3.2-Model-Checkpoints.html">0.3.2 Model Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.3.3-GPU-Performance-Metrics.html">0.3.3 GPU Performance Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.4-Exploring-Different-Learning-Rates.html">0.4 Exploring Different Learning Rates</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">0.4.1 Exploring Different Optimizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Boilerplate">Boilerplate</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Jupyter-Extensions">Jupyter Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Imports">Imports</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Initial-Setup">Initial Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Global-Variables">Global Variables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Combigen-Task-Variables">Combigen Task Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Data-Parameters">Data Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Datasets">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Network-Parameters">Network Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Training-Parameters">Training Parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Background">Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#The-Combigen-Task">The Combigen Task</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Setting-Up-the-Graph">Setting Up the Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Make-the-Datasets">Make the Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#TF-Variables">TF Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Model-and-Metrics">Model and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Varying-the-Optimizer">Varying the Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#New-Session-Function">New Session Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Defining-the-Training-Function">Defining the Training Function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Getting-the-Metrics-for-Different-Optimizers">Getting the Metrics for Different Optimizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Saving-the-Data-(If-it-Doesn’t-Exist)">Saving the Data (If it Doesn’t Exist)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#The-Legend">The Legend</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Training-Set-Metrics">Training Set Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Validation-Set-Metrics">Validation Set Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Plotting-Optimizer-Performance-for-500-Epochs">Plotting Optimizer Performance for 500 Epochs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Stochastic-Gradient-Descent">Stochastic Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Adam-Optimizer">Adam Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Adadelta-Optimizer">Adadelta Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#RMSProp-Optimizer">RMSProp Optimizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Adam-Optimizer-Learning-Rates">Adam Optimizer Learning Rates</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Adam-with-0.0033-LR">Adam with 0.0033 LR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Adam-with-0.033-LR">Adam with 0.033 LR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Adam-with-0.001-LR">Adam with 0.001 LR</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Adam-with-0.00033-LR">Adam with 0.00033 LR</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#RMSProp-Optimizer-Learning-Rates">RMSProp Optimizer Learning Rates</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#RMSProp-Training-Curves-for-LR-0.00033,-0.001,-0.0033,-0.01">RMSProp Training Curves for LR 0.00033, 0.001, 0.0033, 0.01</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Adadelta-Optimizer-Learning-Rates">Adadelta Optimizer Learning Rates</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Plots-for-Adadelta-LRs-1.0,-1.33,-1.67,-2.0">Plots for Adadelta LRs 1.0, 1.33, 1.67, 2.0</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="0.5-Adam-Optimizer-Learning-Rate-Tuning-and-Dropout.html">0.5 Adam Optimizer Learning Rate Tuning and Dropout</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.5.1-Adam-With-Non-Uniform-Task-Statistics.html">0.5.1 Adam with Non-Uniform Task Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.5.2-Adam-With-Non-Uniform-Training-Statistics.html">0.5.2 Adam with Non-Uniform Training Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.6-Hebbian-Learning.html">0.6 Hebbian Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.7-Replicating-Results-with-the-Updated-Task.html">0.7 Replicating Results with the Updated Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.7.1-Managing-Technical-Debt.html">0.7.1 Managing Technical Debt</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.8-Hierarchical-Learning.html">0.8 Hierarchical Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">Project Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://apra93.github.io/leabra-tf/">Home</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/apra93/leabra-tf/">Github</a></li>
<li class="toctree-l1"><a class="reference external" href="https://travis-ci.org/apra93/leabra-tf">Travis CI</a></li>
<li class="toctree-l1"><a class="reference external" href="https://codecov.io/gh/apra93/leabra-tf">Codecov</a></li>
<li class="toctree-l1"><a class="reference external" href="https://app.codacy.com/project/apra93/leabra-tf/dashboard">Codacy</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Leabra Tensorflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>0.4.1 Exploring Different Optimizers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/0.4.1-Exploring-Different-Optimizers.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="0.4.1-Exploring-Different-Optimizers">
<h1>0.4.1 Exploring Different Optimizers<a class="headerlink" href="#0.4.1-Exploring-Different-Optimizers" title="Permalink to this headline">¶</a></h1>
<p>The next easy change to explore the effects of are to use different optimizers on the model to see how it changes.</p>
<div class="section" id="Boilerplate">
<h2>Boilerplate<a class="headerlink" href="#Boilerplate" title="Permalink to this headline">¶</a></h2>
<p>The following subsections are largely boilerplate code, so skip around as needed.</p>
<div class="section" id="Jupyter-Extensions">
<h3>Jupyter Extensions<a class="headerlink" href="#Jupyter-Extensions" title="Permalink to this headline">¶</a></h3>
<p>Load <a class="reference external" href="https://github.com/rasbt/watermark">watermark</a> to see the state of the machine and environment that’s running the notebook. To make sense of the options, take a look at the <a class="reference external" href="https://github.com/rasbt/watermark#usage">usage</a> section of the readme.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load `watermark` extension</span>
<span class="o">%</span><span class="k">load_ext</span> watermark
<span class="c1"># Display the status of the machine and packages. Add more as necessary.</span>
<span class="o">%</span><span class="k">watermark</span> -v -n -m -g -b -t -p numpy,matplotlib,seaborn,tensorflow
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Thu Mar 14 2019 16:46:14

CPython 3.6.8
IPython 7.3.0

numpy 1.16.2
matplotlib 3.0.3
seaborn 0.9.0
tensorflow 1.12.0

compiler   : GCC 7.3.0
system     : Linux
release    : 4.4.0-130-generic
machine    : x86_64
processor  : x86_64
CPU cores  : 12
interpreter: 64bit
Git hash   : 1500006d066cc495368df672c9309801f822f376
Git branch : master
</pre></div></div>
</div>
<p>Load <a class="reference external" href="https://ipython.org/ipython-doc/3/config/extensions/autoreload.html">autoreload</a> which will always reload modules marked with <code class="docutils literal notranslate"><span class="pre">%aimport</span></code>.</p>
<p>This behavior can be inverted by running <code class="docutils literal notranslate"><span class="pre">autoreload</span> <span class="pre">2</span></code> which will set everything to be auto-reloaded <em>except</em> for modules marked with <code class="docutils literal notranslate"><span class="pre">%aimport</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load `autoreload` extension</span>
<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="c1"># Set autoreload behavior</span>
<span class="o">%</span><span class="k">autoreload</span> 1
</pre></div>
</div>
</div>
<p>Load <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> in one of the more <code class="docutils literal notranslate"><span class="pre">jupyter</span></code>-friendly <a class="reference external" href="https://ipython.readthedocs.io/en/stable/interactive/plotting.html">rich-output modes</a>. Some options (that may or may not have worked) are <code class="docutils literal notranslate"><span class="pre">inline</span></code>, <code class="docutils literal notranslate"><span class="pre">notebook</span></code>, and <code class="docutils literal notranslate"><span class="pre">gtk</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Set the matplotlib mode.</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="section" id="Imports">
<h3>Imports<a class="headerlink" href="#Imports" title="Permalink to this headline">¶</a></h3>
<p>Static imports that shouldn’t necessarily change throughout the notebook.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[72]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Standard library imports</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="k">import</span> <span class="n">Path</span>

<span class="c1"># Third party</span>
<span class="kn">import</span> <span class="nn">IPython</span> <span class="k">as</span> <span class="nn">ipy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
<p>Local imports that may or may not be autoreloaded. This section contains things that will likely have to be re-imported multiple times, and have additions or subtractions made throughout the project.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Task script</span>
<span class="o">%</span><span class="k">aimport</span> leabratf.tasks.combinatorics.combigen
<span class="kn">import</span> <span class="nn">leabratf.tasks.combinatorics.combigen</span> <span class="k">as</span> <span class="nn">cg</span>
<span class="c1"># Visualization for the task</span>
<span class="o">%</span><span class="k">aimport</span> leabratf.visualization.combigen_heatmap
<span class="kn">import</span> <span class="nn">leabratf.visualization.combigen_heatmap</span> <span class="k">as</span> <span class="nn">cgh</span>
<span class="c1"># Metrics visulaization</span>
<span class="o">%</span><span class="k">aimport</span> leabratf.visualization.metrics
<span class="kn">import</span> <span class="nn">leabratf.visualization.metrics</span> <span class="k">as</span> <span class="nn">plt_metrics</span>
<span class="c1"># Utility functions</span>
<span class="o">%</span><span class="k">aimport</span> leabratf.utils
<span class="kn">from</span> <span class="nn">leabratf.utils</span> <span class="k">import</span> <span class="n">setup_logging</span>
<span class="o">%</span><span class="k">aimport</span> leabratf.constants
<span class="kn">from</span> <span class="nn">leabratf.constants</span> <span class="k">import</span> <span class="n">DIR_DATA_PROC</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Initial-Setup">
<h3>Initial Setup<a class="headerlink" href="#Initial-Setup" title="Permalink to this headline">¶</a></h3>
<p>Set <a class="reference external" href="https://seaborn.pydata.org/generated/seaborn.set.html">seaborn defaults</a> for matplotlib.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Set up the logger configuration to something more useful than baseline. Creates log files for the different log levels in the <code class="docutils literal notranslate"><span class="pre">logs</span></code> directory.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">logging.yml</span></code> for the exact logging configuration.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[66]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Run base logger setup</span>
<span class="n">setup_logging</span><span class="p">()</span>
<span class="c1"># Define a logger object</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;leabratf&#39;</span><span class="p">)</span>
<span class="c1"># Don&#39;t propagate messages</span>
<span class="n">logger</span><span class="o">.</span><span class="n">propagate</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Global-Variables">
<h3>Global Variables<a class="headerlink" href="#Global-Variables" title="Permalink to this headline">¶</a></h3>
<div class="section" id="Combigen-Task-Variables">
<h4>Combigen Task Variables<a class="headerlink" href="#Combigen-Task-Variables" title="Permalink to this headline">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Number of slots in a training set</span>
<span class="n">STACK</span> <span class="o">=</span> <span class="mi">4</span>
<span class="c1"># Size of each axis in the input array</span>
<span class="n">SIZE</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># Number of axes to use per slot</span>
<span class="n">DIMS</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># Number of lines per axis</span>
<span class="n">LINES</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data-Parameters">
<h4>Data Parameters<a class="headerlink" href="#Data-Parameters" title="Permalink to this headline">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Number of epochs to train for</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">500</span>
<span class="c1"># Number of samples in the training set</span>
<span class="n">N_TRAIN</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># Number of samples in the validation set</span>
<span class="n">N_VAL</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># Number of samples in the testing set</span>
<span class="n">N_TEST</span> <span class="o">=</span> <span class="mi">500</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Datasets">
<h4>Datasets<a class="headerlink" href="#Datasets" title="Permalink to this headline">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Training Data</span>
<span class="n">Y_TRAIN</span> <span class="o">=</span> <span class="n">cg</span><span class="o">.</span><span class="n">generate_labels</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N_TRAIN</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="n">STACK</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="n">DIMS</span><span class="p">,</span> <span class="n">n_lines</span><span class="o">=</span><span class="n">LINES</span><span class="p">)</span>
<span class="n">X_TRAIN</span> <span class="o">=</span> <span class="n">cg</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Y_TRAIN</span><span class="p">)</span>
<span class="c1"># Validation Data</span>
<span class="n">Y_VAL</span> <span class="o">=</span> <span class="n">cg</span><span class="o">.</span><span class="n">generate_labels</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N_VAL</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="n">STACK</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="n">DIMS</span><span class="p">,</span> <span class="n">n_lines</span><span class="o">=</span><span class="n">LINES</span><span class="p">)</span>
<span class="n">X_VAL</span> <span class="o">=</span> <span class="n">cg</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Y_VAL</span><span class="p">)</span>
<span class="c1"># Testing data</span>
<span class="n">Y_TEST</span> <span class="o">=</span> <span class="n">cg</span><span class="o">.</span><span class="n">generate_labels</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N_TEST</span><span class="p">,</span> <span class="n">stack</span><span class="o">=</span><span class="n">STACK</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="n">DIMS</span><span class="p">,</span> <span class="n">n_lines</span><span class="o">=</span><span class="n">LINES</span><span class="p">)</span>
<span class="n">X_TEST</span> <span class="o">=</span> <span class="n">cg</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Y_TEST</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Network-Parameters">
<h4>Network Parameters<a class="headerlink" href="#Network-Parameters" title="Permalink to this headline">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Learning rate</span>
<span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># Batch size</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># Number of parameters in the inputs</span>
<span class="n">N_INPUTS</span> <span class="o">=</span> <span class="n">STACK</span> <span class="o">*</span> <span class="n">SIZE</span> <span class="o">**</span> <span class="n">DIMS</span>
<span class="c1"># Number of hidden units</span>
<span class="n">N_HIDDEN_1</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># Number of parameters in the labels</span>
<span class="n">N_OUTPUTS</span> <span class="o">=</span> <span class="n">STACK</span> <span class="o">*</span> <span class="n">SIZE</span> <span class="o">*</span> <span class="n">DIMS</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training-Parameters">
<h4>Training Parameters<a class="headerlink" href="#Training-Parameters" title="Permalink to this headline">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Number of times to print an update</span>
<span class="n">N_UPDATES</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># Which device to train on</span>
<span class="n">TF_DEVICE</span> <span class="o">=</span> <span class="s1">&#39;/cpu:0&#39;</span>
<span class="c1"># Number of models to train with</span>
<span class="n">N_MODELS</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="Background">
<h2>Background<a class="headerlink" href="#Background" title="Permalink to this headline">¶</a></h2>
<p>This section goes over some of the background information for the notebook using results from previous notebooks. Skip around as needed.</p>
<div class="section" id="The-Combigen-Task">
<h3>The Combigen Task<a class="headerlink" href="#The-Combigen-Task" title="Permalink to this headline">¶</a></h3>
<p>Quickly remind ourselves what the task looks like before diving in.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">cgh</span><span class="o">.</span><span class="n">visualize_combigen</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_32_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_32_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Setting-Up-the-Graph">
<h2>Setting Up the Graph<a class="headerlink" href="#Setting-Up-the-Graph" title="Permalink to this headline">¶</a></h2>
<p>This next section will define the computational graph that will be used to generate the metrics down below. It is largely code copied from nb-0.3, so skip around as needed.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Cleanup any residual nodes</span>
<span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="Make-the-Datasets">
<h3>Make the Datasets<a class="headerlink" href="#Make-the-Datasets" title="Permalink to this headline">¶</a></h3>
<p>Define the various <code class="docutils literal notranslate"><span class="pre">tf.Dataset</span></code>s that will be used.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">make_tf_datasets</span><span class="p">(</span><span class="n">init_ops</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># The first step of the setup is that each of the datasets (training, validation, and</span>
    <span class="c1"># testing) are turned into their own `Dataset` objects.</span>
    <span class="c1"># Training dataset</span>
    <span class="n">dataset_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
        <span class="p">(</span><span class="n">X_TRAIN</span><span class="p">,</span> <span class="n">Y_TRAIN</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="c1"># Validation dataset</span>
    <span class="n">dataset_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
        <span class="p">(</span><span class="n">X_VAL</span><span class="p">,</span> <span class="n">Y_VAL</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">N_VAL</span><span class="p">)</span>
    <span class="c1"># Testing dataset</span>
    <span class="n">dataset_test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
        <span class="p">(</span><span class="n">X_TEST</span><span class="p">,</span> <span class="n">Y_TEST</span><span class="p">))</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">N_TEST</span><span class="p">)</span>

    <span class="c1"># Next, let&#39;s define the iterators for each of the datasets, and then add their</span>
    <span class="c1"># initializations to the `init_ops` list.</span>
    <span class="c1"># Training iterator</span>
    <span class="n">train_iter</span> <span class="o">=</span> <span class="n">dataset_train</span><span class="o">.</span><span class="n">make_initializable_iterator</span><span class="p">()</span>
    <span class="c1"># Validation iterator</span>
    <span class="n">val_iter</span> <span class="o">=</span> <span class="n">dataset_val</span><span class="o">.</span><span class="n">make_initializable_iterator</span><span class="p">()</span>
    <span class="c1"># Testing iterator</span>
    <span class="n">test_iter</span> <span class="o">=</span> <span class="n">dataset_test</span><span class="o">.</span><span class="n">make_initializable_iterator</span><span class="p">()</span>
    <span class="c1"># Aggregate the iterators</span>
    <span class="n">iterators</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">]</span>

    <span class="c1"># Add the initiatlizations to the init opts</span>
    <span class="n">init_ops</span> <span class="o">=</span> <span class="n">init_ops</span> <span class="ow">or</span> <span class="p">[]</span>
    <span class="n">init_ops</span> <span class="o">+=</span> <span class="p">[</span><span class="n">train_iter</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span> <span class="n">val_iter</span><span class="o">.</span><span class="n">initializer</span><span class="p">,</span> <span class="n">test_iter</span><span class="o">.</span><span class="n">initializer</span><span class="p">]</span>

    <span class="c1"># And finally, the interesting part. Rather than creating separate next elements for</span>
    <span class="c1"># the model, the `tf.data` API has a string handler iterator so we can contextually</span>
    <span class="c1"># switch the active `Dataset` object, resulting in different values being used for `x`</span>
    <span class="c1"># and `y`.</span>

    <span class="c1"># The way this is done is by defining a `tf.placeholder` variable, which is used</span>
    <span class="c1"># first to create a string handler iterator, and later to hold the dataset-indicating</span>
    <span class="c1"># string handle. The string handler iterator is what then changes the values of `x` and</span>
    <span class="c1"># `y`, naturally also supplying them using the `get_next` method.</span>
    <span class="c1"># The placeholder variable of type string</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[])</span>
    <span class="c1"># Iterator from string handle</span>
    <span class="n">handle_iterator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Iterator</span><span class="o">.</span><span class="n">from_string_handle</span><span class="p">(</span>
        <span class="n">handle</span><span class="p">,</span> <span class="n">dataset_train</span><span class="o">.</span><span class="n">output_types</span><span class="p">,</span>
        <span class="n">dataset_train</span><span class="o">.</span><span class="n">output_shapes</span><span class="p">)</span>

    <span class="c1"># x and y that will be used in the graph</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">handle_iterator</span><span class="o">.</span><span class="n">get_next</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">iterators</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">init_ops</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="TF-Variables">
<h3>TF Variables<a class="headerlink" href="#TF-Variables" title="Permalink to this headline">¶</a></h3>
<p>Straight forward section where we define the weights and biases. One thing to note is that the weights are initialized using the <code class="docutils literal notranslate"><span class="pre">tf.contrib.layers.xavier_initializer</span></code>.</p>
<p>Additionally, create an empty list that will contain the initialization operations to be performed at the start of a session.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Weights and biases</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;h1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_h1&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">N_INPUTS</span><span class="p">,</span> <span class="n">N_HIDDEN_1</span><span class="p">],</span>
                      <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">()),</span>
    <span class="s1">&#39;out&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;w_out&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">N_HIDDEN_1</span><span class="p">,</span> <span class="n">N_OUTPUTS</span><span class="p">],</span>
                      <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">xavier_initializer</span><span class="p">()),</span>
<span class="p">}</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;b1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_1&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">N_HIDDEN_1</span><span class="p">],</span>
                      <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()),</span>
    <span class="s1">&#39;out&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;b_out&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">N_OUTPUTS</span><span class="p">],</span>
                      <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()),</span>
<span class="p">}</span>

<span class="n">init_ops</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Model-and-Metrics">
<h3>Model and Metrics<a class="headerlink" href="#Model-and-Metrics" title="Permalink to this headline">¶</a></h3>
<p>The architecture is the same as previous notebooks. See <code class="docutils literal notranslate"><span class="pre">nb-0.3</span></code> for more details.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">oreilly_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="c1"># Reshape for hidden layer</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_INPUTS</span><span class="p">])</span>
    <span class="c1"># Single hidden layer</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s1">&#39;h1&#39;</span><span class="p">]),</span> <span class="n">biases</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]))</span>
    <span class="c1"># Output layer</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]),</span> <span class="n">biases</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">])</span>
    <span class="c1"># Reshape for labels</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">STACK</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">DIMS</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">TF_DEVICE</span><span class="p">):</span>
    <span class="c1"># Get the relevant dataset nodes</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">iterators</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">init_ops</span> <span class="o">=</span> <span class="n">make_tf_datasets</span><span class="p">(</span><span class="n">init_ops</span><span class="p">)</span>

    <span class="c1"># Build the model</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">oreilly_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Ensure y is cast to the same type as logits</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># We will keep this in for now</span>
    <span class="c1"># Define alpha as placeholder variable</span>
    <span class="n">alpha_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[])</span>

    <span class="c1"># Define loss and optimizer</span>
    <span class="n">loss_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">))</span>

    <span class="c1"># Define some intermediate nodes</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">rounded_predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="n">equal_labels_and_preds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">rounded_predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="c1"># O&#39;Reilly Accuracy</span>
    <span class="n">axis_acc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">equal_labels_and_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">slot_acc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">axis_acc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">sample_acc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">slot_acc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Elemental Accuracy</span>
    <span class="n">el_acc_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">equal_labels_and_preds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="c1"># Axis Accuracy</span>
    <span class="n">axis_acc_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">axis_acc</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="c1"># Slot Accuracy</span>
    <span class="n">slot_acc_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">slot_acc</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="c1"># Sample Accuracy</span>
    <span class="n">sample_acc_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">sample_acc</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Varying-the-Optimizer">
<h3>Varying the Optimizer<a class="headerlink" href="#Varying-the-Optimizer" title="Permalink to this headline">¶</a></h3>
<p>Let’s create several different training operations.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[100]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">TF_DEVICE</span><span class="p">):</span>
    <span class="c1"># Adam</span>
    <span class="n">train_op_adam</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">alpha_ph</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_op</span><span class="p">)</span>
    <span class="c1"># Adadelta</span>
    <span class="n">train_op_adadelta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdadeltaOptimizer</span><span class="p">(</span><span class="n">alpha_ph</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_op</span><span class="p">)</span>
    <span class="c1"># RMSProp</span>
    <span class="n">train_op_rmsprop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">alpha_ph</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_op</span><span class="p">)</span>

<span class="c1"># And now add these to a dictionary</span>
<span class="n">optimizer_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Adam&#39;</span> <span class="p">:</span> <span class="n">train_op_adam</span><span class="p">,</span>
    <span class="s1">&#39;Adadelta&#39;</span> <span class="p">:</span> <span class="n">train_op_adadelta</span><span class="p">,</span>
    <span class="s1">&#39;RMSProp&#39;</span> <span class="p">:</span> <span class="n">train_op_rmsprop</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="New-Session-Function">
<h3>New Session Function<a class="headerlink" href="#New-Session-Function" title="Permalink to this headline">¶</a></h3>
<p>In the event that we do not want to immediately close sessions, we won’t be using the context handler but will still need to grab new sessions as necessary. So let’s quickly write a function that will properly run <code class="docutils literal notranslate"><span class="pre">sess.close()</span></code> if a previous session exists and then return a new <code class="docutils literal notranslate"><span class="pre">tf.Session</span></code> instance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">new_session</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Try to close the globally defined session if it isn&#39;t already</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sess</span><span class="o">.</span><span class="n">_closed</span><span class="p">:</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="c1"># If it doesn&#39;t exist, then just pass</span>
    <span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="c1"># Return the new instance</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Defining-the-Training-Function">
<h3>Defining the Training Function<a class="headerlink" href="#Defining-the-Training-Function" title="Permalink to this headline">¶</a></h3>
<p>Let’s define a function that runs the training routine and accepts the number of epochs as the inputs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">train_with_optimizer</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">train_op</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">n_updates</span><span class="o">=</span><span class="n">N_UPDATES</span><span class="p">,</span>
                         <span class="n">init_ops</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Ensure this is an int</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="c1"># Dict with the various metrics we care about</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:[],</span> <span class="s1">&#39;el_acc&#39;</span><span class="p">:[],</span> <span class="s1">&#39;ax_acc&#39;</span><span class="p">:[],</span> <span class="s1">&#39;sl_acc&#39;</span><span class="p">:[],</span> <span class="s1">&#39;sm_acc&#39;</span><span class="p">:[],</span>
               <span class="s1">&#39;val_loss&#39;</span><span class="p">:[],</span> <span class="s1">&#39;val_el_acc&#39;</span><span class="p">:[],</span> <span class="s1">&#39;val_ax_acc&#39;</span><span class="p">:[],</span> <span class="s1">&#39;val_sl_acc&#39;</span><span class="p">:[],</span>
               <span class="s1">&#39;val_sm_acc&#39;</span><span class="p">:[]}</span>
    <span class="c1"># Run the initialization ops</span>
    <span class="n">init_ops</span> <span class="o">=</span> <span class="n">init_ops</span> <span class="ow">or</span> <span class="p">[]</span>
    <span class="n">init_ops</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">(),</span> <span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">()]</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_ops</span><span class="p">)</span>
    <span class="c1"># Define training and validation handlers</span>
    <span class="n">training_handle</span><span class="p">,</span> <span class="n">validation_handle</span><span class="p">,</span> <span class="n">testing_handle</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">string_handle</span><span class="p">()</span>
                                                                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">ep_loss</span><span class="p">,</span> <span class="n">ep_el_acc</span><span class="p">,</span> <span class="n">ep_sl_acc</span><span class="p">,</span> <span class="n">ep_ax_acc</span><span class="p">,</span> <span class="n">ep_sm_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="n">N_TRAIN</span><span class="p">])</span>
        <span class="c1"># Run the training steps</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_TRAIN</span><span class="p">):</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">ep_loss</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ep_el_acc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ep_sl_acc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ep_ax_acc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ep_sm_acc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="p">[</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss_op</span><span class="p">,</span> <span class="n">el_acc_op</span><span class="p">,</span> <span class="n">axis_acc_op</span><span class="p">,</span> <span class="n">slot_acc_op</span><span class="p">,</span> <span class="n">sample_acc_op</span><span class="p">],</span>
                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">handle</span><span class="p">:</span> <span class="n">training_handle</span><span class="p">,</span> <span class="n">alpha_ph</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>

        <span class="c1"># Get means for the epoch</span>
        <span class="n">epoch_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">ep_loss</span><span class="p">,</span> <span class="n">ep_el_acc</span><span class="p">,</span> <span class="n">ep_sl_acc</span><span class="p">,</span> <span class="n">ep_ax_acc</span><span class="p">,</span> <span class="n">ep_sm_acc</span><span class="p">),</span>
                                  <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Calculate validation accuracy and loss</span>
        <span class="n">val_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">loss_op</span><span class="p">,</span> <span class="n">el_acc_op</span><span class="p">,</span> <span class="n">axis_acc_op</span><span class="p">,</span> <span class="n">slot_acc_op</span><span class="p">,</span> <span class="n">sample_acc_op</span><span class="p">],</span>
                                 <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">handle</span><span class="p">:</span> <span class="n">validation_handle</span><span class="p">}))</span>

        <span class="c1"># Record</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">epoch_data</span> <span class="o">+</span> <span class="n">val_data</span><span class="p">):</span>
            <span class="n">metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># Selectively display the epoch number</span>
        <span class="k">if</span> <span class="n">n_updates</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">/</span> <span class="n">n_updates</span><span class="p">)</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Completed epoch </span><span class="si">{0}</span><span class="s2">/</span><span class="si">{1}</span><span class="s2">. Metrics:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>
                   <span class="s2">&quot;                     Loss   Sample Accuracy   Elem Accuracy</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>
                   <span class="s2">&quot;    Epoch:      </span><span class="si">{2:10.4f}</span><span class="s2">   </span><span class="si">{3:10.4f}</span><span class="s2">   </span><span class="si">{4:10.4f}</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>
                   <span class="s2">&quot;    Validation: </span><span class="si">{5:10.4f}</span><span class="s2">   </span><span class="si">{6:10.4f}</span><span class="s2">   </span><span class="si">{7:10.4f}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">epoch_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">epoch_data</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">epoch_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">val_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">val_data</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">val_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># Calculate accuracy for test images</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimization Finished! Testing Sample Accuracy:&quot;</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">sample_acc_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">handle</span><span class="p">:</span> <span class="n">testing_handle</span><span class="p">}))</span>
    <span class="k">return</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Getting-the-Metrics-for-Different-Optimizers">
<h2>Getting the Metrics for Different Optimizers<a class="headerlink" href="#Getting-the-Metrics-for-Different-Optimizers" title="Permalink to this headline">¶</a></h2>
<p>Let’s now run the function defined with two learning rates smaller than what is standardly used, and two larger.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">new_session</span><span class="p">()</span>
<span class="n">optimizer_metrics</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">train_op</span> <span class="ow">in</span> <span class="n">optimizer_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Starting training using the &quot;</span><span class="si">{0}</span><span class="s1">&quot; optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">N_MODELS</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_with_optimizer</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">train_op</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">,</span>
                                                <span class="n">n_updates</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">init_ops</span><span class="o">=</span><span class="n">init_ops</span><span class="p">))</span>

        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Encountered a KeyboardInterrupt. Starting a IPython Shell.&#39;</span><span class="p">)</span>
            <span class="n">ipy</span><span class="o">.</span><span class="n">embed</span><span class="p">()</span>
            <span class="n">inp</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">while</span> <span class="n">inp</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">]:</span>
                <span class="n">inp</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Continue training? [y/n]&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Continuing training...&#39;</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Exiting.&#39;</span><span class="p">)</span>
                <span class="n">exit</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>
    <span class="k">if</span> <span class="n">exit</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Completed training using the &quot;</span><span class="si">{0}</span><span class="s1">&quot; optimizer!</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="n">optimizer_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting training using the &#34;Adam&#34; optimizer
Optimization Finished! Testing Sample Accuracy: 0.858
Optimization Finished! Testing Sample Accuracy: 0.804
Optimization Finished! Testing Sample Accuracy: 0.82
Optimization Finished! Testing Sample Accuracy: 0.77
Optimization Finished! Testing Sample Accuracy: 0.818
Optimization Finished! Testing Sample Accuracy: 0.8
Optimization Finished! Testing Sample Accuracy: 0.824
Optimization Finished! Testing Sample Accuracy: 0.8
Optimization Finished! Testing Sample Accuracy: 0.816
Optimization Finished! Testing Sample Accuracy: 0.82
Completed training using the &#34;Adam&#34; optimizer!

Starting training using the &#34;Adadelta&#34; optimizer
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Completed training using the &#34;Adadelta&#34; optimizer!

Starting training using the &#34;RMSProp&#34; optimizer
Optimization Finished! Testing Sample Accuracy: 0.618
Optimization Finished! Testing Sample Accuracy: 0.676
Optimization Finished! Testing Sample Accuracy: 0.644
Optimization Finished! Testing Sample Accuracy: 0.646
Optimization Finished! Testing Sample Accuracy: 0.592
Optimization Finished! Testing Sample Accuracy: 0.618
Optimization Finished! Testing Sample Accuracy: 0.6
Optimization Finished! Testing Sample Accuracy: 0.56
Optimization Finished! Testing Sample Accuracy: 0.654
Optimization Finished! Testing Sample Accuracy: 0.618
Completed training using the &#34;RMSProp&#34; optimizer!

CPU times: user 1h 36min 16s, sys: 17min 57s, total: 1h 54min 14s
Wall time: 42min 21s
</pre></div></div>
</div>
<p>The training output above appears to not have completed, but this is because the jupyter-lab client was was shut down, leaving the host to continue processing. This means that output to the cell was suspended.</p>
<div class="section" id="Saving-the-Data-(If-it-Doesn’t-Exist)">
<h3>Saving the Data (If it Doesn’t Exist)<a class="headerlink" href="#Saving-the-Data-(If-it-Doesn’t-Exist)" title="Permalink to this headline">¶</a></h3>
<p>Let’s save the data we got above if it doesn’t already exist. First make the optimizer directory if it doesn’t exist.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Make this directory if it doesn&#39;t exist</span>
<span class="n">optimizer_dir</span> <span class="o">=</span> <span class="n">DIR_DATA_PROC</span> <span class="o">/</span> <span class="s1">&#39;different_optimizers&#39;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">optimizer_dir</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="n">optimizer_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Now loop through each of the resulting dataframes, check if there is already a file named according to its parameters, and then selectively save it.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[87]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">save_data</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">n_models</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">,</span> <span class="n">n_train</span><span class="o">=</span><span class="n">N_TRAIN</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">exp_dir</span><span class="o">=</span><span class="n">DIR_DATA_PROC</span><span class="p">,</span> <span class="n">dryrun</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">df_metrics</span> <span class="o">=</span> <span class="n">metrics</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="k">else</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="c1"># Grab the exp params</span>
    <span class="n">n_models</span> <span class="o">=</span> <span class="n">n_models</span> <span class="ow">or</span> <span class="n">df_metrics</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_epochs</span> <span class="o">=</span> <span class="n">n_epochs</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_metrics</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Create a dict containing different keys and values if they differ</span>
    <span class="c1"># from the default exp</span>
    <span class="n">file_name_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># If we used a different number of models</span>
    <span class="k">if</span> <span class="n">n_models</span> <span class="o">!=</span> <span class="n">N_MODELS</span><span class="p">:</span>
        <span class="n">file_name_dict</span><span class="p">[</span><span class="s1">&#39;models&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_models</span><span class="p">)</span>
    <span class="c1"># If we used a different number of epochs</span>
    <span class="k">if</span> <span class="n">n_epochs</span> <span class="o">!=</span> <span class="n">EPOCHS</span><span class="p">:</span>
        <span class="n">file_name_dict</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>
    <span class="c1"># If we used a different number of training samples</span>
    <span class="k">if</span> <span class="n">n_train</span> <span class="o">!=</span> <span class="n">N_TRAIN</span><span class="p">:</span>
        <span class="n">file_name_dict</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_train</span><span class="p">)</span>
    <span class="c1"># If we used a different learning rate</span>
    <span class="k">if</span> <span class="nb">float</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ALPHA</span><span class="p">:</span>
        <span class="n">file_name_dict</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="c1"># If we used a different optimizer</span>
    <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">!=</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
        <span class="n">file_name_dict</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="c1"># Create the filename and path object</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">file_name_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">file_name</span><span class="p">:</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">val</span><span class="p">,</span> <span class="n">key</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="s1">&#39;_&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">file_name</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">)])</span>
    <span class="n">file_name</span> <span class="o">+=</span> <span class="s1">&#39;.csv&#39;</span>
    <span class="n">exp_file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">exp_dir</span><span class="p">)</span> <span class="o">/</span> <span class="n">file_name</span>

    <span class="c1"># Save the df if the file does not exist</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">exp_file</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dryrun</span><span class="p">:</span>
            <span class="n">df_metrics</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exp_file</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Saved df to &#39;</span><span class="si">{0}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exp_file</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dryrun: (Not) Saved df to &#39;</span><span class="si">{0}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exp_file</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;File &#39;</span><span class="si">{0}</span><span class="s2">&#39; present, skipping.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exp_file</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[82]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimizer_data</span> <span class="ow">in</span> <span class="n">optimizer_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">save_data</span><span class="p">(</span><span class="n">optimizer_data</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">exp_dir</span><span class="o">=</span><span class="n">optimizer_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/adam_optimizer.csv&#39; present, skipping.
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/adadelta_optimizer.csv&#39; present, skipping.
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/rmsprop_optimizer.csv&#39; present, skipping.
</pre></div></div>
</div>
<p>In the event that the data needs to be reloaded (for example if the kernel needs to be restarted), I have the data saved locally and can be reloaded using the following lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">leabratf.constants</span> <span class="k">import</span> <span class="n">DIR_DATA_PROC</span>
<span class="n">optimizer_dir</span> <span class="o">=</span> <span class="n">DIR_DATA_PROC</span> <span class="o">/</span> <span class="s1">&#39;different_optimizers&#39;</span>
<span class="n">optimizer_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">optimizer_dir</span><span class="o">.</span><span class="n">iterdir</span><span class="p">():</span>
    <span class="n">optimizer_split_name</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
    <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_split_name</span><span class="p">[</span>
        <span class="n">optimizer_split_name</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;optimizer&#39;</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">optimizer_dict</span><span class="p">[</span><span class="n">optimizer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">file</span><span class="p">),</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>One last useful piece of data to collect is to create a symbolic link to the data collected in <code class="docutils literal notranslate"><span class="pre">nb-0.3.1</span></code> and use that as the data for the model trained using sgd. So let’s create a symbolic link to it that preserves the naming scheme we have for this notebook.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Directory that has the training set data</span>
<span class="n">training_set_dir</span> <span class="o">=</span> <span class="n">DIR_DATA_PROC</span> <span class="o">/</span> <span class="s1">&#39;training_set_sizes&#39;</span>
<span class="c1"># This should exist</span>
<span class="k">assert</span> <span class="n">training_set_dir</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>

<span class="c1"># Now grab the file that contains the desired data</span>
<span class="n">training_set_op_file</span> <span class="o">=</span> <span class="n">training_set_dir</span> <span class="o">/</span> <span class="s1">&#39;10_models_25000_epochs_100_training_samples.csv&#39;</span>
<span class="c1"># This should also exist</span>
<span class="k">assert</span> <span class="n">training_set_op_file</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>

<span class="c1"># Build the filename we want to use for the symbolic link</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">optimizer_dir</span> <span class="o">/</span> <span class="s2">&quot;25000_epochs.csv&quot;</span>

<span class="c1"># Create the sym link if the file does not exist</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">file_path</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="n">file_path</span><span class="o">.</span><span class="n">symlink_to</span><span class="p">(</span><span class="n">training_set_op_file</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Created symlink from </span><span class="si">{0}</span><span class="s2"> to </span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">file_path</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">training_set_op_file</span><span class="p">)))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;File &#39;</span><span class="si">{0}</span><span class="s2">&#39; present, skipping.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">file_path</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Created symlink from /home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/25000_epochs.csv to /home/abdullah_rashed/work/projects/leabra-tf/data/processed/training_set_sizes/10_models_25000_epochs_100_training_samples.csv
</pre></div></div>
</div>
</div>
<div class="section" id="The-Legend">
<h3>The Legend<a class="headerlink" href="#The-Legend" title="Permalink to this headline">¶</a></h3>
<p>Before plotting everything, let’s remind ourselves of the metrics legend. This is a direct copy from <code class="docutils literal notranslate"><span class="pre">nb-0.3.1</span></code>, so skip as needed.</p>
<div class="section" id="Training-Set-Metrics">
<h4>Training Set Metrics<a class="headerlink" href="#Training-Set-Metrics" title="Permalink to this headline">¶</a></h4>
<p>Metrics obtained every epoch from from performance on the training set.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">loss</span></code> - Loss for the training set</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">el_acc</span></code> - Element-wise accuracy between predictions and labels for the training set</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ax_acc</span></code> - Axis accuracy for each slot in each sample between predictions and labels for the training set. Score per sample goes in eighth steps between 0.0 and 1.0. Every correct axis in every slot contributes 0.125 to the the overall accuracy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sl_acc</span></code> - Slot accuracy for sample between predictions and labels for the training set. Score per sample goes in quarter steps between 0.0 and 1.0. Any slot that has all elements correct contributes 0.25 to the score</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sm_acc</span></code> - Sample accuracy between predictions and labels for the training set. Score per sample is binary - model must get every element correct to receive 1.0</p></li>
</ul>
</div>
<div class="section" id="Validation-Set-Metrics">
<h4>Validation Set Metrics<a class="headerlink" href="#Validation-Set-Metrics" title="Permalink to this headline">¶</a></h4>
<p>Metrics obtained every epoch from from performance on the validation set.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">val_loss</span></code> - Loss for the validation set for a particular epoch</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">val_el_acc</span></code> - Element-wise accuracy between predictions and labels for the validation set. See above for details</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">val_sl_acc</span></code> - Slot accuracy for sample between predictions and labels for the validation set. See above for details</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">val_sm_acc</span></code> - Sample accuracy between predictions and labels for the training set. See above for details</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="Plotting-Optimizer-Performance-for-500-Epochs">
<h2>Plotting Optimizer Performance for 500 Epochs<a class="headerlink" href="#Plotting-Optimizer-Performance-for-500-Epochs" title="Permalink to this headline">¶</a></h2>
<p>Now let’s plot all the training curves for the different optimizers. The plotting function here is the same as in <code class="docutils literal notranslate"><span class="pre">nb-0.4</span></code>.</p>
<div class="section" id="Stochastic-Gradient-Descent">
<h3>Stochastic Gradient Descent<a class="headerlink" href="#Stochastic-Gradient-Descent" title="Permalink to this headline">¶</a></h3>
<p>Just to remind ourselves of what the previous training curves looked like, let’s plot this first. Feel free to skip to the next sections.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[59]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load the data</span>
<span class="n">df_sgd_metric</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">file_path</span><span class="p">),</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">df_sgd_metric</span><span class="p">,</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Training Curves Using SGD&#39;</span><span class="p">,</span>
                            <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_62_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_62_0.png" />
</div>
</div>
</div>
<div class="section" id="Adam-Optimizer">
<h3>Adam Optimizer<a class="headerlink" href="#Adam-Optimizer" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[63]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">optimizer_metrics</span><span class="p">[</span><span class="s1">&#39;Adam&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Training Curves Using Adam&#39;</span><span class="p">,)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_64_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_64_0.png" />
</div>
</div>
</div>
<div class="section" id="Adadelta-Optimizer">
<h3>Adadelta Optimizer<a class="headerlink" href="#Adadelta-Optimizer" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[64]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">optimizer_metrics</span><span class="p">[</span><span class="s1">&#39;Adadelta&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Training Curves Using Adadelta&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_66_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_66_0.png" />
</div>
</div>
</div>
<div class="section" id="RMSProp-Optimizer">
<h3>RMSProp Optimizer<a class="headerlink" href="#RMSProp-Optimizer" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[65]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">optimizer_metrics</span><span class="p">[</span><span class="s1">&#39;RMSProp&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Training Curves Using RMSProp&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_68_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_68_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Adam-Optimizer-Learning-Rates">
<h2>Adam Optimizer Learning Rates<a class="headerlink" href="#Adam-Optimizer-Learning-Rates" title="Permalink to this headline">¶</a></h2>
<p>The adam optimizer had the most promising results. Let’s see how it does with a few different learning rates when run for 500 epochs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[112]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">exp_optimizer_lr</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="p">[</span><span class="n">ALPHA</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">n_models</span><span class="o">=</span><span class="n">N_MODELS</span><span class="p">):</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">new_session</span><span class="p">()</span>
    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">exit</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lrs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Starting &quot;</span><span class="si">{0}</span><span class="s1">&quot; optimizer training with lr of </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">lr</span><span class="p">))</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_models</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_with_optimizer</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">train_op</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
                                                    <span class="n">n_updates</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">init_ops</span><span class="o">=</span><span class="n">init_ops</span><span class="p">))</span>

            <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Encountered a KeyboardInterrupt. Starting a IPython Shell.&#39;</span><span class="p">)</span>
                <span class="n">ipy</span><span class="o">.</span><span class="n">embed</span><span class="p">()</span>
                <span class="n">inp</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
                <span class="k">while</span> <span class="n">inp</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">]:</span>
                    <span class="n">inp</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Continue training or skip current lr? [(Y)es/(N)o/(S)kip]&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">inp</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Continuing training...&#39;</span><span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">elif</span> <span class="n">inp</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;s&#39;</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Continuing to next lr...&#39;</span><span class="p">)</span>
                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Exiting&#39;</span><span class="p">)</span>
                    <span class="n">exit</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
        <span class="k">if</span> <span class="n">exit</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Completed &quot;</span><span class="si">{0}</span><span class="s1">&quot; optimizer training with lr of </span><span class="si">{1}</span><span class="s1">!</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">lr</span><span class="p">))</span>
        <span class="n">metrics_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics_dict</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[74]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0033</span><span class="p">,</span> <span class="mf">0.033</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">adam_alpha_metrics</span> <span class="o">=</span> <span class="n">exp_optimizer_lr</span><span class="p">(</span><span class="n">optimizer_dict</span><span class="p">[</span><span class="s1">&#39;Adam&#39;</span><span class="p">],</span> <span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="n">lrs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting &#34;Adam&#34; optimizer training with lr of 0.0033
Optimization Finished! Testing Sample Accuracy: 0.82
Optimization Finished! Testing Sample Accuracy: 0.816
Optimization Finished! Testing Sample Accuracy: 0.824
Optimization Finished! Testing Sample Accuracy: 0.796
Optimization Finished! Testing Sample Accuracy: 0.814
Optimization Finished! Testing Sample Accuracy: 0.808
Optimization Finished! Testing Sample Accuracy: 0.812
Optimization Finished! Testing Sample Accuracy: 0.814
Optimization Finished! Testing Sample Accuracy: 0.842
Optimization Finished! Testing Sample Accuracy: 0.802
Optimization Finished! Testing Sample Accuracy: 0.806
Completed &#34;Adam&#34; optimizer training with lr of 0.0033!

Starting &#34;Adam&#34; optimizer training with lr of 0.033
Optimization Finished! Testing Sample Accuracy: 0.044
Optimization Finished! Testing Sample Accuracy: 0.04
Optimization Finished! Testing Sample Accuracy: 0.036
Optimization Finished! Testing Sample Accuracy: 0.02
Optimization Finished! Testing Sample Accuracy: 0.044
Optimization Finished! Testing Sample Accuracy: 0.04
Optimization Finished! Testing Sample Accuracy: 0.058
Optimization Finished! Testing Sample Accuracy: 0.032
Optimization Finished! Testing Sample Accuracy: 0.04
Optimization Finished! Testing Sample Accuracy: 0.042
Optimization Finished! Testing Sample Accuracy: 0.048
Completed &#34;Adam&#34; optimizer training with lr of 0.033!

Starting &#34;Adam&#34; optimizer training with lr of 0.1
Optimization Finished! Testing Sample Accuracy: 0.0
Encountered a KeyboardInterrupt. Starting a IPython Shell.
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)
Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information
IPython 7.3.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [1]:  quit
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Continue training? [y/n] n
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Exiting.
CPU times: user 1h 9min 55s, sys: 13min 23s, total: 1h 23min 19s
Wall time: 31min 52s
</pre></div></div>
</div>
<p>Since it didn’t seem to do well with the LR of 0.033, and then had a sample accuracy of 0.0 for the first model when using a LR of 0.1, I ended the exp with the data for just the first two.</p>
<p>Let’s plot both curves.</p>
<div class="section" id="Adam-with-0.0033-LR">
<h3>Adam with 0.0033 LR<a class="headerlink" href="#Adam-with-0.0033-LR" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[76]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">adam_alpha_metrics</span><span class="p">[</span><span class="s1">&#39;0.0033&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adam Training Curves with 0.0033 LR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_74_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_74_0.png" />
</div>
</div>
<p>This learning rate appears to work quite well, and its accuracy is generally better than all the</p>
</div>
<div class="section" id="Adam-with-0.033-LR">
<h3>Adam with 0.033 LR<a class="headerlink" href="#Adam-with-0.033-LR" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">adam_alpha_metrics</span><span class="p">[</span><span class="s1">&#39;0.033&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adam Training Curves with 0.033 LR&#39;</span><span class="p">,)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_77_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_77_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[89]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">lr_name</span><span class="p">,</span> <span class="n">optimizer_data</span> <span class="ow">in</span> <span class="n">adam_alpha_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">save_data</span><span class="p">(</span><span class="n">optimizer_data</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr_name</span><span class="p">,</span>
              <span class="n">exp_dir</span><span class="o">=</span><span class="n">optimizer_dir</span><span class="p">,</span> <span class="n">dryrun</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.0033_lr_adam_optimizer.csv&#39;
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.033_lr_adam_optimizer.csv&#39;
</pre></div></div>
</div>
</div>
<div class="section" id="Adam-with-0.001-LR">
<h3>Adam with 0.001 LR<a class="headerlink" href="#Adam-with-0.001-LR" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[90]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">adam_alpha_metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">**</span><span class="n">adam_alpha_metrics</span><span class="p">,</span>
    <span class="o">**</span><span class="n">exp_optimizer_lr</span><span class="p">(</span><span class="n">optimizer_dict</span><span class="p">[</span><span class="s1">&#39;Adam&#39;</span><span class="p">],</span> <span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="p">[</span><span class="mf">0.001</span><span class="p">]),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting &#34;Adam&#34; optimizer training with lr of 0.001
Optimization Finished! Testing Sample Accuracy: 0.806
Optimization Finished! Testing Sample Accuracy: 0.822
Optimization Finished! Testing Sample Accuracy: 0.78
Optimization Finished! Testing Sample Accuracy: 0.82
Optimization Finished! Testing Sample Accuracy: 0.776
Optimization Finished! Testing Sample Accuracy: 0.828
Optimization Finished! Testing Sample Accuracy: 0.814
Optimization Finished! Testing Sample Accuracy: 0.802
Optimization Finished! Testing Sample Accuracy: 0.8
Optimization Finished! Testing Sample Accuracy: 0.836
Optimization Finished! Testing Sample Accuracy: 0.786
Completed &#34;Adam&#34; optimizer training with lr of 0.001!

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[91]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">adam_alpha_metrics</span><span class="p">[</span><span class="s1">&#39;0.001&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adam Training Curves with 0.001 LR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_81_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_81_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">lr_name</span><span class="p">,</span> <span class="n">optimizer_data</span> <span class="ow">in</span> <span class="n">adam_alpha_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">save_data</span><span class="p">(</span><span class="n">optimizer_data</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr_name</span><span class="p">,</span>
              <span class="n">exp_dir</span><span class="o">=</span><span class="n">optimizer_dir</span><span class="p">,</span> <span class="n">dryrun</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.0033_lr_adam_optimizer.csv&#39; present, skipping.
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.033_lr_adam_optimizer.csv&#39; present, skipping.
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.001_lr_adam_optimizer.csv&#39;
</pre></div></div>
</div>
</div>
<div class="section" id="Adam-with-0.00033-LR">
<h3>Adam with 0.00033 LR<a class="headerlink" href="#Adam-with-0.00033-LR" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[93]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">adam_alpha_metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">**</span><span class="n">adam_alpha_metrics</span><span class="p">,</span>
    <span class="o">**</span><span class="n">exp_optimizer_lr</span><span class="p">(</span><span class="n">optimizer_dict</span><span class="p">[</span><span class="s1">&#39;Adam&#39;</span><span class="p">],</span> <span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="p">[</span><span class="mf">0.00033</span><span class="p">]),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting &#34;Adam&#34; optimizer training with lr of 0.00033
Optimization Finished! Testing Sample Accuracy: 0.758
Optimization Finished! Testing Sample Accuracy: 0.758
Optimization Finished! Testing Sample Accuracy: 0.776
Optimization Finished! Testing Sample Accuracy: 0.76
Optimization Finished! Testing Sample Accuracy: 0.782
Optimization Finished! Testing Sample Accuracy: 0.758
Optimization Finished! Testing Sample Accuracy: 0.766
Optimization Finished! Testing Sample Accuracy: 0.758
Optimization Finished! Testing Sample Accuracy: 0.784
Optimization Finished! Testing Sample Accuracy: 0.742
Optimization Finished! Testing Sample Accuracy: 0.742
Completed &#34;Adam&#34; optimizer training with lr of 0.00033!

CPU times: user 33min 29s, sys: 6min 24s, total: 39min 54s
Wall time: 15min 16s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[95]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">adam_alpha_metrics</span><span class="p">[</span><span class="s1">&#39;0.001&#39;</span><span class="p">],</span>
                            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adam Training Curves with 0.001 LR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_85_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_85_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[96]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">lr_name</span><span class="p">,</span> <span class="n">optimizer_data</span> <span class="ow">in</span> <span class="n">adam_alpha_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">save_data</span><span class="p">(</span><span class="n">optimizer_data</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr_name</span><span class="p">,</span>
              <span class="n">exp_dir</span><span class="o">=</span><span class="n">optimizer_dir</span><span class="p">,</span> <span class="n">dryrun</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.0033_lr_adam_optimizer.csv&#39; present, skipping.
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.033_lr_adam_optimizer.csv&#39; present, skipping.
File &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.001_lr_adam_optimizer.csv&#39; present, skipping.
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/11_models_0.00033_lr_adam_optimizer.csv&#39;
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="RMSProp-Optimizer-Learning-Rates">
<h2>RMSProp Optimizer Learning Rates<a class="headerlink" href="#RMSProp-Optimizer-Learning-Rates" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[102]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0033</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.00033</span><span class="p">]</span>
<span class="n">rmsprop_alpha_metrics</span> <span class="o">=</span> <span class="n">exp_optimizer_lr</span><span class="p">(</span><span class="n">optimizer_dict</span><span class="p">[</span><span class="s1">&#39;RMSProp&#39;</span><span class="p">],</span> <span class="s1">&#39;RMSProp&#39;</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="n">lrs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting &#34;RMSProp&#34; optimizer training with lr of 0.0033
Optimization Finished! Testing Sample Accuracy: 0.634
Optimization Finished! Testing Sample Accuracy: 0.636
Optimization Finished! Testing Sample Accuracy: 0.67
Optimization Finished! Testing Sample Accuracy: 0.636
Optimization Finished! Testing Sample Accuracy: 0.63
Optimization Finished! Testing Sample Accuracy: 0.664
Optimization Finished! Testing Sample Accuracy: 0.644
Optimization Finished! Testing Sample Accuracy: 0.66
Optimization Finished! Testing Sample Accuracy: 0.622
Optimization Finished! Testing Sample Accuracy: 0.63
Completed &#34;RMSProp&#34; optimizer training with lr of 0.0033!

Starting &#34;RMSProp&#34; optimizer training with lr of 0.001
Optimization Finished! Testing Sample Accuracy: 0.63
Optimization Finished! Testing Sample Accuracy: 0.626
Optimization Finished! Testing Sample Accuracy: 0.612
Optimization Finished! Testing Sample Accuracy: 0.646
Optimization Finished! Testing Sample Accuracy: 0.57
Optimization Finished! Testing Sample Accuracy: 0.622
Optimization Finished! Testing Sample Accuracy: 0.618
Optimization Finished! Testing Sample Accuracy: 0.626
Optimization Finished! Testing Sample Accuracy: 0.62
Optimization Finished! Testing Sample Accuracy: 0.644
Completed &#34;RMSProp&#34; optimizer training with lr of 0.001!

Starting &#34;RMSProp&#34; optimizer training with lr of 0.00033
Optimization Finished! Testing Sample Accuracy: 0.532
Optimization Finished! Testing Sample Accuracy: 0.576
Optimization Finished! Testing Sample Accuracy: 0.554
Optimization Finished! Testing Sample Accuracy: 0.556
Optimization Finished! Testing Sample Accuracy: 0.57
Optimization Finished! Testing Sample Accuracy: 0.596
Optimization Finished! Testing Sample Accuracy: 0.576
Optimization Finished! Testing Sample Accuracy: 0.524
Optimization Finished! Testing Sample Accuracy: 0.55
Optimization Finished! Testing Sample Accuracy: 0.56
Completed &#34;RMSProp&#34; optimizer training with lr of 0.00033!

CPU times: user 1h 28min 59s, sys: 17min 3s, total: 1h 46min 3s
Wall time: 40min 26s
</pre></div></div>
</div>
<p>Now let’s save the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[105]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">lr_name</span><span class="p">,</span> <span class="n">optimizer_data</span> <span class="ow">in</span> <span class="n">rmsprop_alpha_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">save_data</span><span class="p">(</span><span class="n">optimizer_data</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">lr</span><span class="o">=</span><span class="n">lr_name</span><span class="p">,</span>
              <span class="n">exp_dir</span><span class="o">=</span><span class="n">optimizer_dir</span><span class="p">,</span>
              <span class="n">dryrun</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/0.0033_lr_rmsprop_optimizer.csv&#39;
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/0.001_lr_rmsprop_optimizer.csv&#39;
Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/0.00033_lr_rmsprop_optimizer.csv&#39;
</pre></div></div>
</div>
<p>And then add in the plots we got earlier for the LR of 0.01.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[109]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">rmsprop_alpha_metrics</span><span class="p">[</span><span class="s1">&#39;0.01&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer_metrics</span><span class="p">[</span><span class="s1">&#39;RMSProp&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="RMSProp-Training-Curves-for-LR-0.00033,-0.001,-0.0033,-0.01">
<h3>RMSProp Training Curves for LR 0.00033, 0.001, 0.0033, 0.01<a class="headerlink" href="#RMSProp-Training-Curves-for-LR-0.00033,-0.001,-0.0033,-0.01" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[108]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Create a sorted list of lrs</span>
<span class="n">lr_sorted_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">rmsprop_alpha_metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()])]</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_sorted_list</span><span class="p">:</span>
    <span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">rmsprop_alpha_metrics</span><span class="p">[</span><span class="n">lr</span><span class="p">],</span>
                                <span class="n">title</span><span class="o">=</span><span class="s1">&#39;RMSProp Training Curves with </span><span class="si">{0}</span><span class="s1"> LR&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_1.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_2.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_2.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_3.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_94_3.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Adadelta-Optimizer-Learning-Rates">
<h2>Adadelta Optimizer Learning Rates<a class="headerlink" href="#Adadelta-Optimizer-Learning-Rates" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[113]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">adadelta_alpha_metrics</span> <span class="o">=</span> <span class="n">exp_optimizer_lr</span><span class="p">(</span><span class="n">optimizer_dict</span><span class="p">[</span><span class="s1">&#39;Adadelta&#39;</span><span class="p">],</span> <span class="s1">&#39;Adadelta&#39;</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="n">lrs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting &#34;Adadelta&#34; optimizer training with lr of 0.1
Optimization Finished! Testing Sample Accuracy: 0.0
Optimization Finished! Testing Sample Accuracy: 0.0
Encountered a KeyboardInterrupt. Starting a IPython Shell.
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)
Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information
IPython 7.3.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [1]:  #Lets skip this lr since it isn&#39;t any more accurate
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [2]:  quite
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-112-fd9ae7e58c17&gt; in exp_optimizer_lr(train_op, optimizer_name, lrs, epochs, n_models)
     10                 metrics.append(train_with_optimizer(sess, train_op, epochs=epochs, lr=lr,
---&gt; 11                                                     n_updates=0, init_ops=init_ops))
     12

&lt;ipython-input-42-b5b668824a55&gt; in train_with_optimizer(sess, train_op, lr, epochs, n_updates, init_ops)
     22                 [train_op, loss_op, el_acc_op, axis_acc_op, slot_acc_op, sample_acc_op],
---&gt; 23                 feed_dict={handle: training_handle, alpha_ph: lr})
     24

~/miniconda3/envs/leabra/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 929                          run_metadata_ptr)
    930       if run_metadata:

~/miniconda3/envs/leabra/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1151       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:

~/miniconda3/envs/leabra/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1328                            run_metadata)
   1329     else:

~/miniconda3/envs/leabra/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-&gt; 1334       return fn(*args)
   1335     except errors.OpError as e:

~/miniconda3/envs/leabra/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-&gt; 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320

~/miniconda3/envs/leabra/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-&gt; 1407         run_metadata)
   1408

KeyboardInterrupt:

During handling of the above exception, another exception occurred:

NameError                                 Traceback (most recent call last)
&lt;ipython-input-2-c70bc754f005&gt; in &lt;module&gt;
----&gt; 1 quite

NameError: name &#39;quite&#39; is not defined

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [3]:  quit
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Continue training or skip current lr? [(Y)es/(N)o/(S)kip] s
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Continuing to next lr...
Completed &#34;Adadelta&#34; optimizer training with lr of 0.1!

Starting &#34;Adadelta&#34; optimizer training with lr of 0.33
Optimization Finished! Testing Sample Accuracy: 0.162
Optimization Finished! Testing Sample Accuracy: 0.158
Optimization Finished! Testing Sample Accuracy: 0.144
Optimization Finished! Testing Sample Accuracy: 0.186
Optimization Finished! Testing Sample Accuracy: 0.15
Encountered a KeyboardInterrupt. Starting a IPython Shell.
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)
Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information
IPython 7.3.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [1]:  # Let&#39;s skip this one as well
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [2]:  quit
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Continue training or skip current lr? [(Y)es/(N)o/(S)kip] S
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Continuing to next lr...
Completed &#34;Adadelta&#34; optimizer training with lr of 0.33!

Starting &#34;Adadelta&#34; optimizer training with lr of 1.0
Optimization Finished! Testing Sample Accuracy: 0.502
Optimization Finished! Testing Sample Accuracy: 0.484
Optimization Finished! Testing Sample Accuracy: 0.544
Optimization Finished! Testing Sample Accuracy: 0.482
Encountered a KeyboardInterrupt. Starting a IPython Shell.
Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)
Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information
IPython 7.3.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
In [1]:  quit
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Continue training or skip current lr? [(Y)es/(N)o/(S)kip] n
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Exiting
CPU times: user 36min 8s, sys: 7min 15s, total: 43min 24s
Wall time: 17min 45s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[114]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%time</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.33</span><span class="p">,</span> <span class="mf">1.67</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>
<span class="n">adadelta_alpha_metrics</span> <span class="o">=</span> <span class="n">exp_optimizer_lr</span><span class="p">(</span><span class="n">optimizer_dict</span><span class="p">[</span><span class="s1">&#39;Adadelta&#39;</span><span class="p">],</span> <span class="s1">&#39;Adadelta&#39;</span><span class="p">,</span> <span class="n">lrs</span><span class="o">=</span><span class="n">lrs</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="o">*</span><span class="n">EPOCHS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Starting &#34;Adadelta&#34; optimizer training with lr of 1.0
Optimization Finished! Testing Sample Accuracy: 0.658
Optimization Finished! Testing Sample Accuracy: 0.62
Optimization Finished! Testing Sample Accuracy: 0.654
Optimization Finished! Testing Sample Accuracy: 0.66
Optimization Finished! Testing Sample Accuracy: 0.614
Optimization Finished! Testing Sample Accuracy: 0.658
Optimization Finished! Testing Sample Accuracy: 0.632
Optimization Finished! Testing Sample Accuracy: 0.638
Optimization Finished! Testing Sample Accuracy: 0.628
Optimization Finished! Testing Sample Accuracy: 0.616
Completed &#34;Adadelta&#34; optimizer training with lr of 1.0!

Starting &#34;Adadelta&#34; optimizer training with lr of 1.33
Optimization Finished! Testing Sample Accuracy: 0.62
Optimization Finished! Testing Sample Accuracy: 0.618
Optimization Finished! Testing Sample Accuracy: 0.668
Optimization Finished! Testing Sample Accuracy: 0.656
Optimization Finished! Testing Sample Accuracy: 0.688
Optimization Finished! Testing Sample Accuracy: 0.66
Optimization Finished! Testing Sample Accuracy: 0.656
Optimization Finished! Testing Sample Accuracy: 0.648
Optimization Finished! Testing Sample Accuracy: 0.622
Optimization Finished! Testing Sample Accuracy: 0.632
Completed &#34;Adadelta&#34; optimizer training with lr of 1.33!

Starting &#34;Adadelta&#34; optimizer training with lr of 1.67
Optimization Finished! Testing Sample Accuracy: 0.658
Optimization Finished! Testing Sample Accuracy: 0.638
Optimization Finished! Testing Sample Accuracy: 0.668
Optimization Finished! Testing Sample Accuracy: 0.65
Optimization Finished! Testing Sample Accuracy: 0.676
Optimization Finished! Testing Sample Accuracy: 0.626
Optimization Finished! Testing Sample Accuracy: 0.658
Optimization Finished! Testing Sample Accuracy: 0.726
Optimization Finished! Testing Sample Accuracy: 0.668
Optimization Finished! Testing Sample Accuracy: 0.678
Completed &#34;Adadelta&#34; optimizer training with lr of 1.67!

Starting &#34;Adadelta&#34; optimizer training with lr of 2.0
Optimization Finished! Testing Sample Accuracy: 0.646
Optimization Finished! Testing Sample Accuracy: 0.664
Optimization Finished! Testing Sample Accuracy: 0.666
Optimization Finished! Testing Sample Accuracy: 0.666
Optimization Finished! Testing Sample Accuracy: 0.674
Optimization Finished! Testing Sample Accuracy: 0.648
Optimization Finished! Testing Sample Accuracy: 0.664
Optimization Finished! Testing Sample Accuracy: 0.686
Optimization Finished! Testing Sample Accuracy: 0.694
Optimization Finished! Testing Sample Accuracy: 0.726
Completed &#34;Adadelta&#34; optimizer training with lr of 2.0!

CPU times: user 10h 18min 47s, sys: 2h 6min 24s, total: 12h 25min 11s
Wall time: 5h 1min 22s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[115]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">lr_name</span><span class="p">,</span> <span class="n">optimizer_data</span> <span class="ow">in</span> <span class="n">adadelta_alpha_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">save_data</span><span class="p">(</span><span class="n">optimizer_data</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;Adadelta&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr_name</span><span class="p">,</span>
              <span class="n">exp_dir</span><span class="o">=</span><span class="n">optimizer_dir</span><span class="p">,</span> <span class="n">dryrun</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Dryrun: (Not) Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/2500_epochs_1.0_lr_adadelta_optimizer.csv&#39;
Dryrun: (Not) Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/2500_epochs_1.33_lr_adadelta_optimizer.csv&#39;
Dryrun: (Not) Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/2500_epochs_1.67_lr_adadelta_optimizer.csv&#39;
Dryrun: (Not) Saved df to &#39;/home/abdullah_rashed/work/projects/leabra-tf/data/processed/different_optimizers/2500_epochs_2.0_lr_adadelta_optimizer.csv&#39;
</pre></div></div>
</div>
<div class="section" id="Plots-for-Adadelta-LRs-1.0,-1.33,-1.67,-2.0">
<h3>Plots for Adadelta LRs 1.0, 1.33, 1.67, 2.0<a class="headerlink" href="#Plots-for-Adadelta-LRs-1.0,-1.33,-1.67,-2.0" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[116]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">lr_sorted_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">adadelta_alpha_metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()])]</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_sorted_list</span><span class="p">:</span>
    <span class="n">plt_metrics</span><span class="o">.</span><span class="n">plot_df_metrics</span><span class="p">(</span><span class="n">adadelta_alpha_metrics</span><span class="p">[</span><span class="n">lr</span><span class="p">],</span>
                                <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Adadelta Training Curves with </span><span class="si">{0}</span><span class="s1"> LR&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_0.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_1.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_2.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_2.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_3.png" src="../_images/notebooks_0.4.1-Exploring-Different-Optimizers_100_3.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="0.5-Adam-Optimizer-Learning-Rate-Tuning-and-Dropout.html" class="btn btn-neutral float-right" title="0.5 Adam Optimizer Learning Rate Tuning and Dropout" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="0.4-Exploring-Different-Learning-Rates.html" class="btn btn-neutral float-left" title="0.4 Exploring Different Learning Rates" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>